<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://ssh022-s.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ssh022-s.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-04-14T14:18:27+00:00</updated><id>https://ssh022-s.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Policy Gradient Algorithms</title><link href="https://ssh022-s.github.io/blog/2020/6pga/" rel="alternate" type="text/html" title="Policy Gradient Algorithms"/><published>2020-12-26T00:32:13+00:00</published><updated>2020-12-26T00:32:13+00:00</updated><id>https://ssh022-s.github.io/blog/2020/6pga</id><content type="html" xml:base="https://ssh022-s.github.io/blog/2020/6pga/"><![CDATA[<h2 id="vpg">VPG</h2> <p><strong>Policy Optimization.</strong> Methods in this family represent a policy explicitly as $\pi_\theta(a|s)$. They optimize the parameters $\theta$ either directly by gradient ascent on the performance objective $J(\pi_\theta)$, or indirectly, by maximizing local approximations of $J(\pi_\theta)$.</p> <p>The performance objective $J(\pi_\theta)$ here is the reward. The reward function is defined as:</p> \[J(\theta) = \sum_{s \in S}d^\pi(s)V^\pi(s) = \sum_{s \in S}d^\pi(s)\sum_{a \in A}\pi_\theta(a|s)Q^\pi(s, a)\] <p>where $d^\pi(s)$ is the stationary distribution of Markov chain for $\pi_\theta$ (on-policy state distribution under $\pi$).</p> <p>Using <em>gradient ascent</em>, we can move θ toward the direction suggested by the gradient $\nabla_\theta J(\theta)$ to find the best θ for $\pi_\theta$ that produces the highest return.</p> <p>VPG Key Equations:</p> <p>The gradient of $J(\theta)$ is</p> \[\nabla_\theta J(\pi_\theta) = \Bbb E_{\pi_\theta}[\nabla_\theta\log{\pi_\theta}(s,a)Q^{\pi_\theta}(s,a)]\] \[\nabla_\theta J(\pi_\theta) =\underset {\tau \sim \pi_\theta} {\Bbb E}[\overset T {\underset {t=0} \sum} \nabla_\theta \log \pi_\theta(a_t|s_t)A^{\pi_\theta}(s_t, a_t)]\] <p>This optimization is almost always performed <strong>on-policy</strong>, which means that each update only uses data collected while acting according to the most recent version of the policy. Policy optimization also usually involves learning an approximator $V_\phi(s)$ for the on-policy value function $V^\pi(s)$, which gets used in figuring out how to update the policy.</p> <p>Policy-based reinforcement learning method Ada:</p> <ul> <li>better convergence properties, just follow the gradient to find the best parameters</li> <li>more effective in high dimensional action spaces</li> <li>can learn stochastic policies, We also get rid of the problem of perceptual aliasing. Perceptual aliasing is when we have two states that seem to be (or actually are) the same, but need different actions.</li> </ul> <p>Dis:</p> <ul> <li> <p>Need a lot of the time, they converge on a local maximum rather than on the global optimum.</p> <p>Solutions: Policy Search</p> </li> </ul> <p>In practice, $V^\pi(s_t)$ cannot be computed exactly, so it has to be approximated. This is usually done with a neural network, $V_\phi(s_t)$, which is updated concurrently with the policy (so that the value network always approximates the value function of the most recent policy).</p> <p>The simplest method for learning , used in most implementations of policy optimization algorithms (including VPG, TRPO, PPO, and A2C), is to minimize a mean-squared-error objective:</p> \[\phi_k = \arg\underset \phi \min \underset {s_t, \hat R_t \sim \pi_k} E [(V_\phi(s_t)-\hat R_t)^2]\] <hr/> <h2 id="ppo">PPO</h2> <p>PPO is motivated by the same question as TRPO: how can we take the biggest possible improvement step on a policy using the data we currently have, without stepping so far that we accidentally cause performance collapse?</p> <p>PPO-Clip doesn’t have a KL-divergence term in the objective and doesn’t have a constraint at all. Instead relies on specialized clipping in the objective function to remove incentives for the new policy to get far from the old policy.</p> <p>PPO-clip updates policies via (by maximizing the objective)</p> \[\theta_{k+1} = \arg \underset \theta \max \underset {s, a \sim \pi_{\theta_k}} E[L(s, a, \theta_k, \theta)]\] <p>$\epsilon$ is a (small) hyperparameter which roughly says how far away the new policy is allowed to go from the old.</p> <p>$L(s, a, \theta_k, \theta)$ is the PPO-Clip objective function:</p> \[L(s,a,\theta_k,\theta) = \min(\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a), g(\epsilon, A^{\pi_{\theta_k}}(s,a))\\ where \ g(\epsilon, A) = \begin{cases} (1+\epsilon)A &amp; {A \geq 0} \\ (1-\epsilon)A &amp; A &lt; 0 \end{cases}\] <p>Cause we want reuse data collect by old policy(after once gradient ascent, the police collected data became old policy). We use Importance sampling to enable use data from different policy.</p> <p>Importance sampling function: $E_{x\sim p}[f(x)] = E_{x\sim p}[\frac {p(x)} {q(x)} f(x)]$. This is where $\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a)$ come from.</p> <p><strong>Advantage is positive</strong>: Suppose the advantage for that state-action pair is positive, in which case its contribution to the objective reduces to</p> \[A \geq 0 \ L(s,a,\theta_k,\theta) = \min(\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}, (1+\epsilon))A^{\pi_{\theta_k}}(s,a)\] <p>Advantage is positive which means this action a under state s is better than others. So we want increase this action’s probability under state s. But the probability incensement can’t great than (1+$\epsilon$).</p> <p><strong>Advantage is negative</strong>: Suppose the advantage for that state-action pair is negative, in which case its contribution to the objective reduces to</p> \[A &lt; 0 \ L(s,a,\theta_k,\theta) = \min(\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}, (1-\epsilon))A^{\pi_{\theta_k}}(s,a)\] <p>Advantage is negative which means this action a under state s is worse than others. So we want decrease this action’s probability under state s. But the probability decrement can’t less than (1+$\epsilon$).</p> <p>SPPO</p> \[L(s,a,\theta_k,\theta) = \min(\frac {\pi_\theta(a|s)} {\pi_{\theta_k}(a|s)}A^{\pi_{\theta_k}}(s,a), g(\epsilon, A^{\pi_{\theta_k}}(s,a)) + \alpha \pi_\theta(\widetilde a|s) \log \pi_\theta(\widetilde a|s)\\ where \ g(\epsilon, A) = \begin{cases} (1+\epsilon)A &amp; {A \geq 0} \\ (1-\epsilon)A &amp; A &lt; 0 \end{cases}\] <hr/> <h2 id="a3c">A3C</h2> <p>Deep neural networks provide rich representations that can enable reinforcement learning (RL) algorithms to perform effectively.</p> <p>the combination of simple online RL algorithms with DNN was fundamentally unstable.</p> <p>Why?</p> <p>the sequence of observed data encountered by an online RL agent is non-stationary, and online RL updates are strongly correlated.</p> <p>Solution: Replay buffer</p> <p>the data can be batched or randomly sampled from different time-steps.</p> <p>reduce non-stationarity and decorrelates updates.</p> <p>Drawbacks:</p> <p>Limits the methods to off-policy RL algorithms.</p> <p>It uses more memory and computation per real interaction</p> <p>This paper:</p> <p>Idea: asynchronously execute multiple agents in parallel, on multiple instances of the environment.</p> <p>enables on-policy and off-policy RL algorithms.</p> <p>run on multi-core CPU. using far less resource.</p> <p>Asynchronous RL Framework</p> <p>multi-threaded asynchronous variants of one-step Sarsa, one-step Q-learning, n-step Q-learning, and advantage actor-critic.</p> <p>two main ideas:</p> <p>First, asynchronous actor-learners, use multiple CPU threads on a single machine.</p> <p>Second, we make the observation that multiple actors-learners running in parallel are likely to be exploring different parts of the environment.</p> <p>Asynchronous advantage actor-critic:</p> <p>We typically use a convolutional neural network that has one softmax output for the policy $\pi(a_t|s_t;\theta)$ and one linear output for the value function $V(s_t;\theta_v)$, with all non-output layers shared.</p> <p>We also found that adding the entropy of the policy $\pi$ to the objective function improved exploration by discouraging premature convergence to suboptimal deterministic policies.</p> <p>asynchronous advantage actor-critic (A3C), maintains a policy $\pi(a_t|s_t;\theta)$ and an estimate of the value function $V(s_t; \theta_v)$. uses the same mix of n-step returns to update both the policy and the value-function. The policy and the value function are updated after every $t_\max$ actions or when a terminal state is reached. The update performed by the algorithm can be seen as $\nabla_{\theta’} \log\pi(a_t | s_t; \theta’)A(s_t, a_t; \theta, \theta_v)$</p> <p>where $A(s_t, a_t;\theta, \theta_v)$ is an estimate of the advantage function given by $\sum_{i=0}^{k-1} \gamma^i r_{t+i} + \gamma^kV(s_{t+k};\theta_v)−V(s_t;\theta_v)$</p> <p>A*3</p> <p><strong>Asynchronous</strong>: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it’s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.</p> <p><strong>Actor-Critic</strong>: Actor-Critic combines the benefits of Q-learning and Policy Gradient approaches. In the case of A3C, our network will estimate both a value function <strong>V(s)</strong> (how good a certain state is to be in) and a policy <strong>π(s)</strong> (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. <strong>Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.</strong></p> <p><strong>Advantage</strong>: $A = Q(s, a) - V(s)$. Since we won’t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage: $A = R - V(s), R = \gamma(r)$</p> <hr/> <h2 id="ddpg-td3">DDPG, TD3</h2> <h3 id="the-optimal-q-function-and-the-optimal-action">The Optimal Q-Function and the Optimal Action</h3> <p>There is an important connection between the optimal action-value function $Q^*(s, a)$ and the action selected by the optimal policy. By definition, $Q^*(s, a)$ gives the expected return for starting in state $s$, taking (arbitrary) action $a$, and then acting according to the optimal policy forever after.</p> <p>The optimal policy in $s$ will select whichever action maximizes the expected return from starting in $s$. As a result, if we have $Q^*$, we can directly obtain the optimal action, $a^*(s)$, via</p> <p>$a^*(s) = \arg \max_aQ^*(s, a)$</p> <p>Note: there may be multiple actions which maximize $Q^*(s, a)$, in which case, all of them are optimal, and the optimal policy may randomly select any of them. But there is always an optimal policy which deterministically selects an action.</p> <h3 id="ddpg">DDPG</h3> <p>Deep Deterministic Policy Gradient (DDPG) is an algorithm which concurrently learns a Q-function and a policy. <strong>It uses off-policy data and the Bellman equation to learn the Q-function, and uses the Q-function to learn the policy.</strong></p> <p>Because the action space is continuous, the function $Q^*(s, a)$ is presumed to be differentiable with respect to the action argument. This allows us to set up an efficient, gradient-based learning rule for a policy $\mu(s)$ which exploits that fact. Then, instead of running an expensive optimization subroutine each time we wish to compute $\max_aQ(s, a)$, we can approximate it with $\max_aQ(s, a) \approx Q(s, \mu(s))$.</p> <p>The Q-Learning Side of DDPG</p> <p>Loss function: <strong>mean-squared Bellman error (MSBE)</strong></p> <p>$L(\phi, D) = E_{(s, a, r, s’, d)\sim D}\left[\left(Q_\phi(s, a)-(r+\gamma(1-d)\max_{a’}Q_\phi(s’, a’))\right)^2\right]$</p> <p>d mean done, if d = 1 (True), s’ is a terminal state.</p> <p>Q-learning algorithms for function approximators, such as DQN (and all its variants) and DDPG, are largely based on minimizing this MSBE loss function.</p> <p>It uses off-policy data and the Bellman equation to learn the Q-function,</p> <p>Again, we can’t compute the $\max_{a’}Q_\phi(s’, a’)$ in continuous action spaces. So we use a <strong>target policy network</strong> to compute an action.</p> <p>The Policy Learning Side of DDPG</p> <p>$L(\phi, D) = E_{(s, a, r, s’, d)\sim D}\left[\left(Q_\phi(s, a)-(r+\gamma(1-d)Q_{\phi_{targ}}(s’, \mu_{\theta_{targ}}(s’)))\right)^2\right]$</p> <p>where $\mu_{\theta_{targ}}$ is the target policy.</p> <p>Policy learning in DDPG is fairly simple. We want to learn a deterministic policy $\mu_\theta(s)$ which gives the action that maximizes $Q_\phi(s, a)$. Because the action space is continuous, and we assume the Q-function is differentiable with respect to action, we can just perform gradient ascent (with respect to policy parameters only) to solve $\max_\theta E_{s\sim D}[Q_\phi(s, \mu_\theta(s))].$</p> <p>Note that the Q-function parameters are treated as constants here.</p> <p>Tricks:</p> <ul> <li><strong>Target Networks.</strong> Q-learning algorithms make use of <strong>target networks</strong>.</li> <li><strong>Replay Buffers.</strong> All standard algorithms for training a deep neural network to approximate $Q^*(s, a)$ make use of an experience replay buffer.</li> <li><strong>OU noise.</strong> To make DDPG policies explore better, we add noise to their actions at training time. The authors of the original DDPG paper recommended time-correlated OU noise, but more recent results suggest that uncorrelated, mean-zero Gaussian noise works perfectly well.</li> <li>For exploration at the start of training. For a fixed number of steps at the beginning (set with the <code class="language-plaintext highlighter-rouge">start_steps</code> keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal DDPG exploration.</li> </ul> <h3 id="td3">TD3</h3> <p><strong>Trick One: Clipped Double-Q Learning.</strong> TD3 learns <em>two</em> Q-functions instead of one (hence “twin”), and uses the smaller of the two Q-values to form the targets in the Bellman error loss functions.</p> <p><strong>Trick Two: “Delayed” Policy Updates.</strong> TD3 updates the policy (and target networks) less frequently than the Q-function. The paper recommends one policy update for every two Q-function updates.</p> <p><strong>Trick Three: Target Policy Smoothing.</strong> TD3 adds noise to the target action, to make it harder for the policy to exploit Q-function errors by smoothing out Q along changes in action.</p> <p>Together, these three tricks result in substantially improved performance over baseline DDPG.</p> <p>TD3 trains a deterministic policy in an off-policy way. To make TD3 policies explore better, we add noise to their actions at training time, typically uncorrelated mean-zero Gaussian noise.</p> <p>For exploration at the start of training. For a fixed number of steps at the beginning (set with the <code class="language-plaintext highlighter-rouge">start_steps</code> keyword argument), the agent takes actions which are sampled from a uniform random distribution over valid actions. After that, it returns to normal TD3 exploration.</p> <hr/> <h3 id="sac">SAC</h3> <p>A central feature of SAC is <strong>entropy regularization.</strong> The policy is trained to maximize a trade-off between expected return and entropy, a measure of randomness in the policy.</p> <p><strong>Entropy-Regularized Reinforcement Learning</strong></p> <p>Let $x$ be a random variable with probability mass or density function $P$. The entropy $H$ of $x$ is computed from its distribution $P$ according to</p> \[H(P) = \underset {x \sim P} E[-\log P(x)].\] <p>The RL problem now:</p> \[\pi^* = \arg \underset \pi \max \underset {\tau \sim \pi} E\left[\overset \infty {\underset {t=0} \sum}\gamma^t\left(R(s_t, a_t, s_{t+1})+\alpha H(\pi(\cdot|s_t))\right) \right]\] <p>where $\alpha &gt; 0$ is the trade-off coefficient.</p> <p>$V^\pi$ is changed to include the entropy bonuses from every timestep:</p> \[V^\pi(s) = \underset {\tau \sim \pi} E \left[\overset \infty {\underset {t=0} \sum}\gamma^t\left(R(s_t, a_t, s_{t+1})+\alpha H(\pi(\cdot|s_t)) \right)|s_0=s \right]\] <p>$Q^\pi$ is changed to include the entropy bonuses from every timestep <em>except the first</em>:</p> \[Q^\pi(s, a) = \underset {\tau \sim \pi} E \left[\overset \infty {\underset {t=0} \sum}\gamma^t R(s_t, a_t, s_{t+1})+\alpha \overset \infty {\underset {t=1} \sum}\gamma^t H(\pi(\cdot|s_t)) |s_0=s,a_0=a \right]\] <p>With these definitons, $V^\pi$ and $Q^\pi$ are connected by:</p> \[V^\pi(s) = \underset {a\sim\pi}E[Q^\pi(s,a)] + \alpha H(\pi(\cdot|s))\] <p>and the Bellman equation for $Q^\pi$ is</p> \[\begin{align} Q^\pi(s,a) &amp;= \underset {s'\sim P}E[R(s,a,s')+\gamma(Q^\pi(s',a')+\alpha H(\pi(\cdot|s')))] \\&amp;= \underset {s'\sim P}E[R(s,a,s')+\gamma V^\pi(s')]. \end{align}\] <p>The modern version that omits the extra value function:</p> <p>SAC concurrently learns a policy $\pi_\theta$ and two Q-functions $Q_{\phi_1}, Q_{\phi_2}$. There are two variants of SAC that are currently standard: one that uses a fixed entropy regularization coefficient $\alpha$, and another that enforces an entropy constraint by varying $\alpha$ over the course of training. For simplicity, Spinning Up makes use of the version with a fixed entropy regularization coefficient, but the entropy-constrained variant is generally preferred by practitioners.</p> <p><strong>Learning Q.</strong> The Q-functions are learned in a similar way to TD3, but with a few key differences.</p> <p>First, what’s similar?</p> <ol> <li>Like in TD3, both Q-functions are learned with MSBE minimization, by regressing to a single shared target.</li> <li>Like in TD3, the shared target is computed using target Q-networks, and the target Q-networks are obtained by polyak averaging the Q-network parameters over the course of training.</li> <li>Like in TD3, the shared target makes use of the <strong>clipped double-Q</strong> trick.</li> </ol> <p>What’s different?</p> <ol> <li>Unlike in TD3, the target also includes a term that comes from SAC’s use of entropy regularization.</li> <li>Unlike in TD3, the next-state actions used in the target come from the <strong>current policy</strong> instead of a target policy.</li> <li>Unlike in TD3, there is no explicit target policy smoothing. TD3 trains a deterministic policy, and so it accomplishes smoothing by adding random noise to the next-state actions. SAC trains a stochastic policy, and so the noise from that stochasticity is sufficient to get a similar effect.</li> </ol> <p>Recursive Bellman equation:</p> \[Q^\pi(s,a) = \underset {s'\sim P, a'\sim\pi} {\mathbb E}[R(s,a,s')+\gamma(Q^\pi(s',a')+\alpha H(\pi(\cdot|s')))]\\ = \underset {s'\sim P, a'\sim\pi} {\mathbb E}[R(s,a,s')+\gamma(Q^\pi(s',a')-\alpha \log\pi(a'|s'))]\] <p>The R H S is an expectation over next states (which come from the replay buffer) and next actions $\tilde a’$ (which come from the current policy). Since it’s an expectation, we can approximate it with samples:</p> \[Q^\pi(s,a) \approx r + \gamma(Q^\pi(s',\tilde a')-\alpha\log\pi(\tilde a'|s')),\tilde a'\sim \pi(\cdot|s')\] <p>the loss functions for the Q-networks in SAC:</p> \[L(\phi_i, \mathcal D) = \underset {(s,a,r,s',d)\sim \mathcal D} E[(Q_{\phi_i}(s,a)-y(r,s',d))^2],\] <p>target is:</p> \[y(r,s',d) = r + \gamma(1-d)(\underset {j=1,2} \min Q_{\phi_{targ,j}}(s',\tilde a')-\alpha\log\pi_\theta(\tilde a'|s')), \tilde a'\sim\pi_\theta(\cdot|s').\] <p><strong>Learning the Policy.</strong> The policy should, in each state, act to maximize the expected future return plus expected future entropy. That is, it should maximize $V^\pi(s)$, which we expand out into</p> \[V^\pi(s) = \underset {a\sim \pi} E[Q^\pi(s,a)]+\alpha H(\pi(\cdot|s))\\ = \underset {a\sim \pi} E[Q^\pi(s,a)-\alpha\log\pi(a|s)].\] <p>The way we optimize the policy makes use of the <strong>reparameterization trick</strong>, in which a sample from $\pi_\theta(\cdot|s)$ is drawn by computing a deterministic function of state, policy parameters, and independent noise:</p> \[\tilde a_\theta(s,\xi) = \tanh(\mu_\theta(s)+\sigma_\theta(s) \odot \xi), \xi\sim\mathcal N(0, I)\] <p>policy loss:</p> \[\underset {j=1,2} \min Q_{\phi_j}(s,\tilde a_\theta(s,\xi))-\alpha\log\pi_\theta(\tilde a_\theta(s,\xi)|s)\] <p>At test time, to see how well the policy exploits what it has learned, we remove stochasticity and use the mean action instead of a sample from the distribution. This tends to improve performance over the original stochastic policy.</p> <hr/>]]></content><author><name></name></author><category term="RL"/><summary type="html"><![CDATA[Introduce Policy Gradient Algorithms includes VPG, PPO, A3C, DDPG, TD3, SAC.]]></summary></entry><entry><title type="html">DQN</title><link href="https://ssh022-s.github.io/blog/2020/7dqn/" rel="alternate" type="text/html" title="DQN"/><published>2020-12-26T00:32:13+00:00</published><updated>2020-12-26T00:32:13+00:00</updated><id>https://ssh022-s.github.io/blog/2020/7dqn</id><content type="html" xml:base="https://ssh022-s.github.io/blog/2020/7dqn/"><![CDATA[<h2 id="dqn">DQN</h2> <p>Goal is maximizes future rewards.</p> <p>Define the future discounted return at time t as $R_t = \sum^T_{t’=t}\gamma^{t’-t}r_{t’}$. T is the time-step at which the games terminates. (Assume the future rewards are discounted by a factor of $\gamma$ per time-step.)</p> <p>Define the optimal action-value function $Q^*(s, a)$ as the maximum expected return. ($\cal E$ : Emulator)</p> \[Q^*(s, a) = \Bbb E_{s' \sim \cal E}[r+\gamma \underset {a'} \max Q^*(s', a')|s, a]\] <p>function approximator to estimate the action-value function $Q(s, a; \theta) \approx Q^*(s, a)$</p> <p>non-linear function approximator: neural network $Q(s, a; \theta) \approx Q^*(s, a)$</p> <p>Loss function $L_i(\theta_i)$</p> \[L_i(\theta_i) = \Bbb E_{s,a \sim \rho(\cdot)}[(y_i - Q(s, a;\theta_i))^2]\\ y_i = \Bbb E_{s' \sim \cal E}[r + \gamma \underset {a'} \max Q(s', a'; \theta_{i-1})|s, a]\] <p>$y_i$ is the target.</p> \[\nabla_{\theta_i}L_i(\theta_i) = \Bbb E_{s,a \sim \rho(\cdot);s'\sim \cal E}[(r+\gamma \underset {a'} \max Q(s', a'; \theta_{i-1})-Q(s, a;\theta_i))\nabla_{\theta_i}Q(s, a;\theta_i)]\] <p>greedy strategy $a = \underset a \max Q(s, a; \theta)$ with probability $1 - \varepsilon $ and selects a random action with probability $\varepsilon$</p> <p>two mechanisms make it work:</p> <p><strong>Fixed Q-targets</strong>:</p> <ul> <li>Using a separate network with a fixed parameter (let’s call it w-) for estimating the TD target.</li> <li>At every $\tau$ step, we copy the parameters from our DQN network to update the target network.</li> </ul> <p><strong>Experience replay</strong>: making more efficient use of observed experience:</p> <ul> <li>Avoid forgetting previous experiences.</li> <li>Reduce correlations between experiences.</li> </ul> <hr/> <h2 id="double-q-learning">Double Q-learning</h2> <p>Q-learning, it is known to sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values.</p> <p>Overoptimize due to estimation errors.</p> <p>The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having to introduce additional networks. <strong>We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value.</strong></p> <p>In reference to both Double Q-learning and DQN, we refer to the resulting algorithm as Double DQN.</p> \[Y_t^{DQN} = R_{t+1}+\gamma \underset a \max Q(S_{t+1}, a; \theta_t^-)\] <p>It’s update is the same as for DQN, but replacing the target $Y_t^{DQN}$ with</p> \[Y_t^{DDQN} = R_{t+1}+\gamma Q(S_{t+1}, \underset a {\arg\max}Q(S_{t+1}, a;\theta_t), \theta_t^-)\] <p>Why Double Q-learning work?</p> <p>Theorem 1 in the paper.</p> <hr/> <h2 id="dueling-dqn">Dueling DQN</h2> <p>The key insight behind our new architecture is that for many states, it is unnecessary to estimate the value of each action choice.</p> <p>The proposed network architecture, which we name the dueling architecture, explicitly separates the representation of state values and (state-dependent) action advantages. The dueling architecture consists of two streams that represent the value and advantage functions, while sharing a common convolutional feature learning module. The two streams are combined via a special aggregating layer to produce an estimate of the state-action value function Q.</p> <p>From the expressions for advantage $Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)$ and state-value $V^\pi(s) = \Bbb E_{a \sim \pi(s)}[Q^\pi(s, a)]$, it follows that $\Bbb E_{a \sim \pi(s)}[A^\pi(s, a)] = 0$. Moreover, for a deterministic policy, $a^* = \arg\max_{a’\in\cal A}Q(s, a’)$, it follows that $Q(s, a^<em>)=V(s)$ and hence $A(s, a^</em>) = 0$.</p> <p>Using the deﬁnition of advantage, we might be tempted to construct the aggregating module as follows:</p> \[Q(s, a;\theta, \alpha, \beta) = V(s;\theta,\beta)+A(s,a;\theta,\alpha) \tag 7\] <p>$V(s;\theta,\beta)$ is a scalar and $A(s,a;\theta,\alpha)$ is a $|\cal A|$-dimensional vector.</p> <p>Equation above is unidentifiable in the sense that given Q we cannot recover V and A uniquely.</p> <p>To address this issue of identifiability, we can force the advantage function estimator to have zero advantage at the chosen action.</p> \[Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta)+ \left(A(s,a;\theta,\alpha)-\underset {a'\in|\cal A|} \max A(s,a';\theta,\alpha)\right) \tag 8\] <p>Now, for $a^<em>=\arg\max_{a’\in\cal A}Q(s,a’;\theta,\alpha,\beta) = \arg\max_{a’\in\cal A}A(s,a’;\theta,\alpha)$, we obtain $Q(s, a^</em>;\theta,\alpha, \beta)=V(s;\theta,\beta)$. Hence, the stream $V(s;\theta,\beta)$ provides an estimate of the value function, while the other stream produces an estimate of the advantage function.</p> <p>An alternative module replaces the max operator with an average:</p> \[Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta)+ \left(A(s,a;\theta,\alpha)- \frac 1 {|\cal A|} \underset {a'} \sum A(s,a';\theta,\alpha)\right) \tag 9\] <p>On the one hand this loses the original semantics of V and A because they are now off-target by a constant, but on the other hand it increases the stability of the optimization: with (9) the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal action’s advantage in (8).</p> <p>The advantage of the dueling architecture lies partly in its ability to learn the state-value function efﬁciently. With every update of the Q values in the dueling architecture, the value stream V is updated – this contrasts with the updates in a single-stream architecture where only the value for one of the actions is updated, the values for all other actions remain untouched. This more frequent updating of the value stream in our approach allocates more resources to V , and thus allows for better approximation of the state values, which in turn need to be accurate for temporal difference-based methods like Q-learning to work.</p> <hr/> <h2 id="prioritized-experience-replay">Prioritized Experience Replay</h2> <p>The key idea is that an RL agent can learn more effectively from some transitions than from others.</p> <p>In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error.</p> <p>TD error $\delta$, which indicates how ‘surprising’ or unexpected the transition is: speciﬁcally, how far the value is from its next-step bootstrap estimate.</p> <p>In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which we correct with importance sampling. Our resulting algorithms are robust and scalable, which we demonstrate on the Atari 2600 benchmark suite, where we obtain faster learning and state-of-the-art performance.</p> <p>we use absolute TD error because it appears to be a reasonable approximation to the “usefulness” of sample</p> <p>a KL-based one in Rainbow DQN</p> <p>We can not keep a tally of all the magnitude of TD errors updated. DeepMind proposes a far more computationally efficient alternative of only updating the $\delta$ terms for items that are actually sampled during the minibatch gradient updates. Since we have to compute $\delta$ anyway to get the loss, we might as well use those to change the priorities.</p> <p>Next, given the absolute TD terms, how do we get a probability distribution for sampling? DeepMind proposes two ways of getting priorities, denoted as $p_i$:</p> <ul> <li>indirect: A <em>rank</em> based method: $p_i = \frac 1 {rank(i)}$ where $rank(i)$ is the rank of transition $i$ when the replay memory is sorted according to $|\delta|$</li> <li>direct: A <em>proportional</em> variant: $p_i = |\delta_i|+\epsilon$, where $\epsilon$ is a small constant ensuring that the sample has some non-zero probability of being drawn.</li> </ul> <p>PER initializes $p_i$ according to the maximum priority of any priority thus far, thus favoring those terms during sampling later.</p> <p>From either of these, we can easily get a probability distribution: $P(i) = \frac {p_i^\alpha} {\sum_k p_k^\alpha}$ The exponent $\alpha$ determines how much prioritization is used, with $\alpha = 0$ corresponding to the uniform case.</p> <p>Since the buffer size N can be quite large (e.g., one million), DeepMind uses special data structures to reduce the time complexity of certain operations. For the proportional-based variant, which is what OpenAI implements, a sum-tree data structure is used to make both updating and sampling O(logN) operations.</p> <p>The estimation of the expected value with stochastic updates relies on those updates corresponding to the same distribution as its expectation. Prioritized replay introduces bias because it changes this distribution in an uncontrolled fashion, and therefore changes the solution that the estimates will converge to (even if the policy and state distribution are fixed). We can correct this bias by using importance-sampling (IS) weights.</p> <p>How do we apply importance sampling? We use the following weights:</p> \[w_i = (\frac 1 N \cdot \frac 1 {P(i)})^\beta\] <p>The $\frac 1 N$ part is because of the current experience replay size.</p> <p>I think the distribution DeepMind is talking about (“same distribution as its expectation”) above is the distribution of samples that are obtained when sampling uniformly at random from the replay buffer.</p> <p><em>PER over-samples those with high priority</em>, so the importance sampling correction should <em>down-weight</em> the impact of the sampled term, which it does by scaling the gradient term so that the gradient has “less impact” on the parameters.</p> <p>the importance sampling in PER is to correct the over-sampling with respect to the uniform distribution.</p> <p>See more in https://danieltakeshi.github.io/2019/07/14/per/</p> <hr/> <h2 id="rainbow">Rainbow</h2> <p>Rainbow =</p> <p>Double DQN + Prioritized experience replay + Dueling network architecture + Learning from multi-step bootstrap targets + Distributional Q-learning + Noisy DQN</p>]]></content><author><name></name></author><category term="RL"/><summary type="html"><![CDATA[DQN and it's improvement.]]></summary></entry><entry><title type="html">Policy Gradient Method</title><link href="https://ssh022-s.github.io/blog/2020/5pg/" rel="alternate" type="text/html" title="Policy Gradient Method"/><published>2020-12-25T17:20:00+00:00</published><updated>2020-12-25T17:20:00+00:00</updated><id>https://ssh022-s.github.io/blog/2020/5pg</id><content type="html" xml:base="https://ssh022-s.github.io/blog/2020/5pg/"><![CDATA[<p>Value Based: A policy was generated directly from the value function e.g. using $\epsilon$-greedy</p> <p>In this lecture we will directly parametrize the policy</p> <p>Advantages:</p> <ul> <li>Better convergence properties</li> <li>Eﬀective in high-dimensional or continuous action spaces</li> <li>Can learn stochastic policies</li> </ul> <p>Disadvantages:</p> <ul> <li>Typically converge to a local rather than global optimum</li> <li>Evaluating a policy is typically ineﬃcient and high variance</li> </ul> <hr/> <p>We consider the case of a stochastic, parameterized policy, $\pi_\theta$. Our goal is given policy $\pi_\theta(s,a)$ with parameters $\theta$, ﬁnd best $\theta$. <strong>But how do we measure the quality of a policy $\pi_\theta$?</strong> We use policy objective functions $J(\theta)$. We aim to maximize the expected return $J(\theta)$.</p> <p>In episodic environments we can use the <strong>start value</strong></p> \[J_1(\theta) = V^{\pi_\theta}(s_1) = \Bbb E_{\pi_\theta}[v_1]\] <p>In continuing environments we can use the <strong>average value</strong></p> \[J_{avV}(\theta) = \underset s \sum d^{\pi_\theta}(s)V^{\pi_\theta}(s)\] <p>Or the <strong>average reward per time-step</strong></p> \[J_{avR}(\theta) = \underset s \sum d^{\pi_\theta}(s) \underset a \sum \pi_\theta(s,a)\cal R^a_s\] <p>where $d^{\pi_\theta}(s)$ is <strong>stationary distribution</strong> of Markov chain for $\pi_\theta$</p> <hr/> <p>Now on, Policy based reinforcement learning is an <strong>optimization</strong> problem.</p> <p>Then Find $\theta$ that maximizes $J(\theta)$ using gradient ascend, eg</p> \[\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta)|_{\theta_k}\] <p>The gradient of policy performance, $\nabla_\theta J(\pi_\theta)$, is called the <strong>policy gradient</strong>, and algorithms that optimize the policy this way are called <strong>policy gradient algorithms.</strong></p> <hr/> <p>Score Function describes how much better or worse it is than other actions on average (relative to the current policy) in state s.</p> <p>Assume policy $\pi_\theta$ is differentiable whenever is non-zero and we know the gradient $\nabla_\theta\pi_\theta(s, a)$</p> <p><strong>Likelihood ratios</strong> exploit the following identity</p> \[\nabla_\theta\pi_\theta(s, a) = \pi_\theta(s,a)\frac {\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)}\\ =\pi_\theta(s, a)\nabla_\theta\log\pi_\theta(s, a)\] <p>The <strong>score function</strong> is $\nabla_\theta\log\pi_\theta(s, a)$</p> <p>in discrete action spaces, Softmax Policy, score function is</p> \[\nabla_\theta\log\pi_\theta(s, a) = \phi(s,a)-\Bbb E_{\pi_\theta}[\phi(s, \cdot)]\] <hr/> <h3 id="policy-gradient-theorem">Policy Gradient Theorem</h3> <p>The policy gradient theorem generalizes the likelihood ratios approach to multi-step MDPs</p> <p>$Q^\pi(s,a)$ here is long-term value, could be reward $R_t$.</p> <blockquote> <p>Theorem</p> <p>For any differentiable policy $\pi_\theta(s, a)$</p> <p>for any of the policy objective functions $J=J_1,J_{avR}, \frac 1 {1-\gamma}J_{avR}$, the policy gradient is \(\nabla_\theta J(\theta) = \Bbb E_{\pi_\theta}[\nabla_\theta\log{\pi_\theta}(s,a)Q^{\pi_\theta}(s,a)]\)</p> </blockquote> <hr/> <h3 id="implementing-the-simplest-policy-gradient">Implementing the Simplest Policy Gradient</h3> <p>(For full code click <a href="https://github.com/openai/spinningup/blob/master/spinup/examples/pg_math/1_simple_pg.py">here</a>.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make core of policy network
</span><span class="n">obs_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">obs_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">obs_ph</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="o">+</span><span class="p">[</span><span class="n">n_acts</span><span class="p">])</span>

<span class="c1"># make action selection op (outputs int actions, sampled from policy)
</span><span class="n">actions</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>This block builds a feedforward neural network categorical policy.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make loss function whose gradient, for the right data, is policy gradient
</span><span class="n">weights_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">act_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">action_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">act_ph</span><span class="p">,</span> <span class="n">n_acts</span><span class="p">)</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">action_masks</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">weights_ph</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">)</span>
</code></pre></div></div> <p>In this block, we build a “loss” function for the policy gradient algorithm. When the right data is plugged in, the gradient of this loss is equal to the policy gradient. The right data means a set of (state, action, weight) tuples collected while acting with environment according to the current policy, where the <strong>weight</strong> for a state-action pair is the reward $R_t$.</p> <hr/> <h3 id="reducing-variance-using-a-critic">Reducing Variance using a Critic</h3> <p>Monte-Carlo policy gradient still has high variance.</p> <p>We use a <strong>critic</strong> to estimate the action-value function</p> \[Q_w(s, a) \approx Q^{\pi_\theta}(s, a)\] <p>Actor-critic algorithms maintain two sets of parameters</p> <p><strong>Critic</strong> Updates action-value function parameters $w$</p> <p><strong>Actor</strong> Updates policy parameters $\theta$, in direction suggested by critic</p> <p>Actor-critic algorithms follow an <em>approximate</em> policy gradient</p> \[\nabla_\theta J(\theta) \approx \Bbb E_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\ Q_w(s,a)] \\ \Delta_\theta = \alpha \nabla_\theta\log \pi_\theta(s,a)\ Q_w(s,a)\] <hr/> <h3 id="bias-in-actor-critic-algorithms">Bias in Actor-Critic Algorithms</h3> <p>we can avoid introducing any bias if we choose value function approximation carefully.</p> <hr/> <h3 id="reducing-variance-using-a-baseline">Reducing Variance using a baseline</h3> <p>because EGLP lemma, We have $\underset {a_t\sim \pi_\theta} E [\nabla_\theta\log\pi_\theta(a_t|s_t)b(s_t)] = 0$</p> <p>This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:</p> \[\nabla_\theta J(\theta) = \Bbb E_{\pi_\theta}\left[\nabla_\theta\log{\pi_\theta}(s,a)\left(Q^{\pi_\theta}(s,a)-b(s)\right)\right]\] <p>A good baseline is the state value function $b(s) = V^{\pi_\theta}(s)$</p> <p>So we can rewrite the policy gradient using the advantage function $A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)$</p> <p>describes how much better or worse it is than other actions on average (relative to the current policy).</p> <hr/> <h3 id="estimating-the-advantage-function">Estimating the Advantage function</h3> <p>The advantage function can signiﬁcantly reduce variance of policy gradient</p> <p>So the critic should really estimate the advantage function</p> <p>For example, by estimating both $V^{\pi_\theta}(s)$ and $Q^{\pi_\theta}(s,a)$</p> <p>Using two function approximators and two parameter vectors,</p> \[V_w(s) \approx V^{\pi_\theta}(s)\\ Q_w(s,a) \approx Q^{\pi_\theta}(s,a)\\ A(s,a) = Q_w(s,a) - V_w(s)\] <p>And updating both value functions by e.g. TD learning</p> <hr/> <p>Reference:</p> <p>Reinforcement Learning An Introduction</p> <p>https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html</p> <p>http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf</p>]]></content><author><name></name></author><category term="RL"/><summary type="html"><![CDATA[Reinforcement Learning basic concepts]]></summary></entry><entry><title type="html">Function Approximation</title><link href="https://ssh022-s.github.io/blog/2020/4fa/" rel="alternate" type="text/html" title="Function Approximation"/><published>2020-11-23T17:20:00+00:00</published><updated>2020-11-23T17:20:00+00:00</updated><id>https://ssh022-s.github.io/blog/2020/4fa</id><content type="html" xml:base="https://ssh022-s.github.io/blog/2020/4fa/"><![CDATA[<p>Introduction</p> <ul> <li>Large-Scale RL</li> <li>Value Function Approximation</li> <li>Which Function Approximator</li> </ul> <p>Incremental Methods</p> <ul> <li> <p>Gradient Descent</p> </li> <li> <p>Value Function Approx. by SGD</p> </li> <li> <p>Feature Vectors, Linear Value Function Approximation</p> <p>Table lookup is a special case of linear value function approximation</p> </li> <li> <p>Incremental Prediction Algorithms</p> </li> <li> <p>MC, TD, TD(gamma), Control with Value Function Approximation</p> </li> <li> <p>(Linear) Action-Value Function Approximation</p> </li> <li> <p>Incremental Control Algorithms</p> </li> <li> <p>Should we Bootstrap. MC performance is really bad</p> </li> <li> <p>Algorithms Convergence</p> </li> </ul> <p>Batch Methods</p> <p>Gradient descent is simple and appealing But it is not sample eﬃcient</p> <p>Batch methods seek to ﬁnd the best ﬁtting value function</p> <p>Given the agent’s experience (“training data”) off policy</p> <ul> <li>Least Squares Prediction</li> <li>SGD with Experience Replay</li> <li>Experience Replay in DQN</li> <li>Linear Least Squares Prediction</li> <li>Least Squares Policy Iteration</li> <li>Least Squares <ul> <li>Action-Value Function Approximation</li> <li>Control</li> <li>Q-Learning</li> <li>Policy Iteration Algorithm</li> </ul> </li> </ul> <h3 id="the-motivation-for-function-approximation-over-table-lookup">the motivation for Function Approximation over Table Lookup</h3> <p>So far we have represented value function by a lookup table. Every state s has an entry $V(s)$ or every state-action pair s,a has an entry $Q(s, a)$.</p> <p>Problem with large MDPs: There are too many states and/or actions to store in memory It is too slow to learn the value of each state individually.</p> <p><strong>Function approximation</strong> because it takes examples from a desired function (e.g., a value function) and attempts to generalize from them to construct an approximation of the entire function.</p> <p>Solution for large MDPs: Estimate value function with <strong>function approximation</strong></p> <p>function approximation</p> <p>$\hat{v}(s, w) \approx v_\pi(s) $ or $\hat{q}(s, a, w) \approx q_\pi(s, a)$</p> <p><em>the number of weights (the dimensionality of w) is much less than the number of states. Generalize from seen states to unseen states. Update parameter w using MC or TD learning.</em></p> <p>Which Function Approximator? We consider diﬀerentiable function approximators like Linear combinations of features and Neural network.</p> <p>Furthermore, we require a training method that is suitable for non-stationary, non-iid data.</p> <hr/> <h3 id="incremental-methods">Incremental Methods</h3> <h4 id="value-function-approx-by-stochastic-gradient-descent">Value Function Approx. By Stochastic Gradient Descent</h4> <p>Goal: ﬁnd parameter vector <strong>w</strong> minimizing mean-squared error between approximate value fn $\hat{v}(s, w)$ and true value fn $v_\pi(s)$</p> \[J(w) = \Bbb E_\pi[(v_\pi(S)-\hat{v}(S, w))^2]\] <p>Gradient descent ﬁnds a local minimum</p> \[\begin{align} \Delta w &amp; = - \frac{1}{2}\alpha\nabla_wJ(w)\\ &amp; =\alpha \Bbb E_\pi[(v_\pi(S)-\hat{v}(S,w))\nabla_\pi\hat{v}(S, w)] \end{align}\] <p>Stochastic gradient descent samples the gradient</p> \[\Delta w = \alpha(v_\pi(S)-\hat{v}(S, w))\nabla_\pi\hat{v}(S, w)\] <p><em>Expected update is equal to full gradient update.</em></p> <h4 id="incremental-prediction-algorithms">Incremental Prediction Algorithms</h4> <p>All of the prediction methods covered here have been described as updates to an estimated value function that shift its value at particular states toward a “backed-up value,” or update target, for that state.</p> <p>Have assumed true value function $v_\pi(s)$ given by supervisor But in RL there is no supervisor, only rewards In practice, we substitute a target for $v_\pi(s)$</p> <p>For MC, the target is the return $G_t$</p> \[\Delta w = \alpha (G_t - \hat{v}(S_t, w))\nabla_w\hat{v}(S_t, w)\] <p>Monte-Carlo evaluation converges to a local optimum Even when using non-linear value function approximation</p> <p>For TD(0), the target is the TD target $R_{t+1}+\gamma\hat{v}(S_{t+1}, w)$</p> \[\Delta w = \alpha (R_{t+1}+\gamma\hat{v}(S_{t+1}, w) - \hat{v}(S_t, w))\nabla_w\hat{v}(S_t, w)\] <p>Linear TD(0) converges (close) to global optimum</p> <p>For TD($\gamma$), the target is the $\gamma$-return $G_t^\gamma$</p> \[\Delta w = \alpha (G_t^\gamma - \hat{v}(S_t, w))\nabla_w\hat{v}(S_t, w)\] <p>TD does not follow the gradient of any objective function. This is why TD can diverge when oﬀ-policy or using non-linear function approximation Gradient TD follows true gradient of projected Bellman error.</p> <p><em>Reinforcement learning with function approximation involves a number of new issues that do not normally arise in conventional supervised learning, such as nonstationary, bootstrapping, and delayed targets.</em></p> <h3 id="linear-methods">Linear Methods</h3> <p>Linear methods approximate the state-value function by the inner product between w and x(s):</p> \[\hat{v}(s, w) \dot= w^Tx(s) \dot=\overset d {\underset {i=1} \sum} w_ix_i(s)\] <hr/> <h3 id="batch-methods">Batch Methods</h3> <p>Gradient descent is simple and appealing But it is not sample eﬃcient.</p> <p>Batch methods seek to ﬁnd the best ﬁtting value function, given the agent’s experience (“training data”) off policy</p> <h4 id="least-squares-prediction">Least squares Prediction</h4> <p>Least squares algorithms ﬁnd parameter vector w minimizing sum-squared error between $\hat{v}(s_t, w)$ and target values $v^\pi_t$</p> \[LS(w) = \overset T {\underset {t=1} \sum}(v^\pi_t-\hat{v}(s_t, w))^2 \\ = \Bbb E_D[(v^\pi-\hat{v}(s, w))^2]\] <p>Given experience consisting of &lt;state,value&gt; pairs , $&lt;s_1, v_1^\pi&gt;$</p> <p>Repeat:</p> <ol> <li> <p>Sample state, value from experience $&lt;s, v^\pi&gt;$</p> </li> <li> <p>Apply stochastic gradient descent update</p> \[\Delta w = \alpha (v^\pi-\hat{v}(s, w))\nabla_w\hat{v}(s, w)\] </li> </ol> <p>Converges to least squares solution</p> \[w^\pi = \underset w {\arg\min}\ LS(w)\] <p>batching using experience replay: Store experience as dataset, randomize it, and repeatedly apply minibatch SGD.</p> <p>Tricks to stabilize non-linear function approximators: Fixed Targets. The target is calculated based on frozen parameter values from a previous time step.</p> <p>DQN uses experience replay and ﬁxed Q-targets.</p> <hr/>]]></content><author><name></name></author><category term="RL"/><summary type="html"><![CDATA[Reinforcement Learning basic concepts]]></summary></entry><entry><title type="html">Model-Free Prediction</title><link href="https://ssh022-s.github.io/blog/2020/3mfp/" rel="alternate" type="text/html" title="Model-Free Prediction"/><published>2020-10-25T17:20:00+00:00</published><updated>2020-10-25T17:20:00+00:00</updated><id>https://ssh022-s.github.io/blog/2020/3mfp</id><content type="html" xml:base="https://ssh022-s.github.io/blog/2020/3mfp/"><![CDATA[<p>Estimate the value function of an unknown MDP.</p> <ul> <li>Monte Carlo Prediction</li> <li>Temporal-Difference (TD) Prediction</li> </ul> <hr/> <p>Monte Carlo Methods</p> <p>Monte Carlo methods require only experience—sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.</p> <ul> <li> <p>MC methods learn directly from episodes of experience</p> </li> <li> <p>MC is model-free: no knowledge of MDP transitions / rewards</p> </li> <li> <p>MC learns from complete episodes: no bootstrapping</p> </li> <li> <p>MC uses the simplest possible idea: value = mean return</p> </li> </ul> <p>Here we deﬁne Monte Carlo methods only for episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected.</p> <p>TD methods</p> <ul> <li> <p>TD methods learn directly from episodes of experience</p> </li> <li> <p>TD is model-free: no knowledge of MDP transitions / rewards</p> </li> <li> <p>TD learns from incomplete episodes, by bootstrapping</p> </li> <li> <p>TD updates a guess towards a guess</p> </li> </ul> <hr/> <h3 id="monte-carlo-prediction">Monte Carlo Prediction</h3> <p>An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value.</p> <p>Goal: learn $v_\pi$ from episodes of experience under policy $\pi$</p> <p>return is the total discounted reward:</p> <p>$G_t = R_{t+1} + \gamma R_{t+2} + … + \gamma^{T-1}R_T$</p> <p>Each average is itself an unbiased estimate, and the standard deviation of its error falls as $1/\sqrt n$, where $n$ is the number of returns averaged. Whereas the DP diagram shows all possible transitions, the Monte Carlo diagram shows only those sampled on the one episode. Monte Carlo methods do not bootstrap. One can generate many sample episodes starting from the states of interest, averaging returns from only these states, ignoring all others.</p> <h3 id="td-prediction">TD Prediction</h3> <p>Firstly, recall the monte carlo method. Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environments is</p> \[V(S_t) \leftarrow V(S_t) + \alpha[G_t-V(S_t)]\] <p>Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(St)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update:</p> \[V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]\] <p>The target for the TD update is <strong>$R_{t+1}+\gamma V(S_{t+1})$</strong></p> <p>Measuring the difference between <strong>the estimated value of $S_t$</strong> and <strong>the better estimate $R_{t+1}+\gamma V(S_{t+1})$</strong>. This quantity, called the $TD\ error$:</p> \[\delta_t\ \dot =\ R_{t+1} + \gamma V(S_{t+1})-V(S_t)\] <h4 id="mc-and-td">MC and TD</h4> <p>Goal: learn $v_\pi$ online from experience under policy $\pi$</p> <p>Incremental every-visit Monte-Carlo</p> <ul> <li> <p>update value $V(S_t)$ toward <em>actual</em> return $G_t$</p> \[V(S_t) \leftarrow V(S_t) + \alpha (G_t-V(S_t))\] </li> </ul> <p>Simplest temporal-difference learning algorithm: TD(0)</p> <ul> <li> <p>Update value $V(S_t)$ toward <em>estimated</em> return $R_{t+1}+\gamma V(S_{t+1})$</p> \[V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1}+\gamma V(S_{t+1})- V(S_t))\] </li> </ul> <h4 id="advantages-and-disadvantages-of-mc-vs-td">Advantages and Disadvantages of MC vs. TD</h4> <p>TD can learn before knowing the final outcome</p> <ul> <li>TD can learn online after every step</li> <li>MC must wait until end of episode before return is known</li> </ul> <p>TD can learn without the final outcome</p> <ul> <li>TD can learn from incomplete sequences</li> <li>MC can only learn from complete sequences</li> <li>TD works in continuing (non-terminating) environments</li> <li>MC only works for episodic (terminating) environments</li> </ul> <p>TD methods have usually been found to converge faster than constant-$\alpha$ MC methods on stochastic tasks</p> <hr/> <h2 id="model-free-control">Model-Free Control</h2> <p>Policy improvement.</p> <ul> <li>Monte-Carlo Control</li> <li>TD Control</li> </ul> <p>Model-Free Policy Iteration Using Action-Value Function.</p> <p>Greedy policy improvement over V(s) requires model of MDP</p> \[\pi'(s) = \underset{a \in A} {\arg\max} \cal R^a_s+\cal P^a_{ss'}V(s')\] <p>Policy improvement is done by making the policy greedy with respect to the current value function. In this case we have an action-value function, and therefore no model is needed to construct the greedy policy.</p> <p>Greedy policy improvement over $Q(s, a)$ is model-free</p> \[\pi'(s) = \underset {a \in A} {\arg \max} Q(s, a)\] <p>problem about greedy action selection: never exploration.</p> <h4 id="epsilon-greedy-exploration">$\epsilon$-Greedy Exploration</h4> <p>$\epsilon$-Greedy, Simplest idea for ensuring continual exploration. meaning that most of the time they choose an action that has maximal estimated action value, but with probability $\epsilon$ they instead select an action at random.</p> <h3 id="monte-carlo-control">Monte-Carlo Control</h3> <h3 id="td-control">TD Control</h3> <p>Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC)</p> <ul> <li>Lower variance</li> <li>Online</li> <li>Incomplete sequences</li> </ul> <p>Natural idea: use TD instead of MC in our control loop</p> <ul> <li>Apply TD to Q(S,A)</li> <li>Use $\epsilon$-greedy policy improvement</li> <li><strong>Update every time-step</strong></li> </ul> <h3 id="sarsa-on-policy-td-control">Sarsa: On-policy TD Control</h3> \[Q(S, A) \leftarrow Q(S, A) + \alpha (R+\gamma Q(S', A')-Q(S,A))\] <p>This update is done after every transition from a nonterminal state St. If $S’$ is terminal, then $Q(S’, A’)$ is defined as zero.</p> <p>n-step Sarsa</p> <p>n = 1 (Sarsa) $q_t^{(1)} = R_{t+1} + \gamma Q(S_{t+1})$</p> <p>n = 2 $q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2})$</p> <p>…</p> <p>n = $\infty$ (MC) $q_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} +\ …\ + \gamma^{T-1}R_T$</p> <p>n-step Q-return $q_t^{(n)} = R_{t+1} + \gamma R_{t+2} +\ …\ + \gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n})$</p> <p>n-step Sarsa updates Q(s,a) towards the n-step Q-return:</p> \[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(q_t^{(n)}-Q(S_t, A_t))\] <p>**Sarsa(λ) Algorithm: **</p> <h4 id="off-policy-learning">Off-Policy Learning</h4> <p>Evaluate target policy $\pi(a|s)$ to compute $v_\pi(s)$ or $q_\pi(s, a)$</p> <p>While following behavior policy $\mu(a|s)$</p> \[\{S_1,A_1,R_2,\ ...,S_T\}\sim \mu\] <p>Re-use experience generated from old policies $\pi_1, \pi_2, \ …, \pi_{t-1}$, Learn from observing humans or other agents.</p> <p>Learn about optimal policy while following exploratory policy</p> <p>Learn about multiple policies while following one policy</p> <h4 id="q-learning-off-policy-td-control">Q-Learning: Off-policy TD Control</h4> <p>consider oﬀ-policy learning of action-values $Q(s, a)$</p> <p>Next action is chosen using behavior policy $A_{t+1} \sim \mu(\cdot|S_t)$</p> <p>But we consider alternative successor action $A’ \sim \pi(\cdot|S_t)$</p> <p>And update $Q(S_t, A_t)$ towards value of alternative action</p> \[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R_{t+1}+\gamma Q(S_{t+1}, A')-Q(S_t, A_t))\] <p>The target policy $\pi$ is greedy w.r.t $Q(s, a)$</p> \[\pi(S_{t+1}) = \underset {a'} {\arg\max} Q(S_{t+1}, a')\] <p>The behavior policy $\mu$ is e.g. $\epsilon$-greedy w.r.t $Q(s, a)$</p> \[\begin{align} &amp;\ \ \ \ \ R_{t+1} + \gamma Q(S_{t+1}, A') \\ &amp; = R_{t+1} + \gamma Q(S_{t+1}, \underset {a'} {\arg\max}Q(S_{t+1}, a')) \\ &amp; = R_{t+1} + \underset {a'} \max \gamma Q(S_{t+1}, a') \end{align}\] <p>Q-Learning Control Algorithm</p> \[Q(S, A) \leftarrow Q(S, A) + \alpha(R+\gamma \underset {a'} \max Q(S', a')-Q(S, A))\] <p>Sarsa (State-Action-Reward-State-Action): The update is based on the action actually taken by the agent. \(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right)\) Sarsa uses the next action a_{t+1} chosen by the current policy, making it an on-policy algorithm. Q-learning: The update is based on the maximum possible action-value from the next state, regardless of the action actually taken. \(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( R_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right)\) Q-learning uses the maximum possible reward from the next state, making it an off-policy algorithm.</p>]]></content><author><name></name></author><category term="RL"/><summary type="html"><![CDATA[Reinforcement Learning basic concepts]]></summary></entry><entry><title type="html">Dynamic Programming</title><link href="https://ssh022-s.github.io/blog/2020/2dp/" rel="alternate" type="text/html" title="Dynamic Programming"/><published>2020-09-09T17:20:00+00:00</published><updated>2020-09-09T17:20:00+00:00</updated><id>https://ssh022-s.github.io/blog/2020/2dp</id><content type="html" xml:base="https://ssh022-s.github.io/blog/2020/2dp/"><![CDATA[<p>What is dynamic programming?</p> <p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to <strong>compute optimal policies</strong> given a <strong>perfect model of the environment as a Markov decision process (MDP)</strong>.</p> <p><strong>Dynamic</strong>: sequential or temporal component to the problem</p> <p><strong>Programming</strong>: optimizing a “program”, i.e. a policy</p> <p>Dynamic programming assumes full knowledge of the MDP</p> <p>Dynamic Programming is a very general solution method for problems which have two properties:</p> <ul> <li>Optimal substructure <ul> <li>Principle of optimality applies</li> <li>Optimal solution can be decomposed into subproblems</li> </ul> </li> <li>Overlapping subproblems <ul> <li>Subproblems recur many times</li> <li>Solutions can be cached and reused</li> </ul> </li> </ul> <p>Markov decision processes satisfy both properties</p> <ul> <li>Bellman equation gives recursive decomposition</li> <li>Value function stores and reuses solutions</li> </ul> <h3 id="policy-evaluation-prediction">Policy Evaluation (Prediction)</h3> <p>We consider how to compute the state-value function $v_\pi$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem.</p> \[v_\pi(s) \dot = \underset a \sum \pi(a|s) \underset {s',r} \sum p(s',r|s,a)[r+\gamma v_\pi(s')]\] <p>where $\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$, and the expectations are subscripted by $\pi$ to indicate that they are conditional on $\pi$ being followed.</p> \[\begin{align} v_{k+1}(s)\ &amp; \dot=\ \Bbb E_\pi[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s]\\ &amp; = \underset a \sum \pi(a|s)\underset {s',r}\sum p(s',r|s,a)[r+\gamma v_k(s')] \end{align}\] <p>The sequence ${v_k}$ can be shown in general to converge to $v_\pi$ as $k\rightarrow \infty$ under the same conditions that guarantee the existence of $v_\pi$.</p> <h3 id="policy-improvement">Policy Improvement</h3> <p>Consider a deterministic policy, $a = \pi(s)$</p> <p>We can <em>improve</em> the policy by acting greedily</p> \[\pi'(s) = \underset {a \in A} {\arg \max}\ q_\pi(s, a)\] <p>Let $\pi$ and $\pi’$ be any pair of deterministic policies such that, for all $s\in S$,</p> \[q_\pi(s, \pi'(s)) \geq v_\pi(s)\] <p>Then the policy $\pi’$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states $s\in S$:</p> \[v_{\pi'} \geq v_\pi(s)\] <p>new <em>greedy</em> policy, $\pi’$, given by</p> \[\begin{align} \pi'(s)\ &amp; \dot =\ \underset a {\arg \max}\ q_{\pi}(s, a)\\ &amp; = \underset a {\arg \max}\ \Bbb E[R_{t+1}+\gamma v_{\pi}(S_{t+1})\ |\ S_t=s, A_t=a]\\ &amp; = \underset a {\arg \max}\ \underset {s',r} \sum p(s',r|s,a)[r+\gamma v_\pi(s')] \end{align}\] <h3 id="value-iteration">Value Iteration</h3> <p>If we know the solution to subproblems $v_<em>(s’)$, then solution $v_</em>(s)$ can be found by one-step lookahead</p> \[v_*(s) \leftarrow \underset {a\in A} \max R_s^a + \gamma \underset {s'\in S}\sum P_{ss'}^av_*(s')\] <p>Value iteration is obtained simply by turning the Bellman optimality equation into an update rule.</p> <p>Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement.</p> <h3 id="asynchronous-dynamic-programming">Asynchronous Dynamic Programming</h3> <p>Three simple ideas for asynchronous dynamic programming:</p> <ul> <li>In-place dynamic programming</li> <li>Prioritized sweeping</li> <li>Real-time dynamic programming</li> </ul> <p><strong>In-place dynamic programming</strong></p> <p>In-place value iteration only stores one copy of value function. that is, with each new value immediately overwriting the old one. It usually converges faster than the two-copies version, because it uses new data as soon as they are available.</p> <p><strong>Prioritized Sweeping</strong></p> <p>Use magnitude of Bellman error to guide state selection. Backup the state with the largest remaining Bellman error. Can be implemented eﬃciently by maintaining a priority queue.</p> <p><strong>Real-time dynamic programming</strong></p> <p>Idea: only states that are relevant to agent.</p> <p>After each time-step agent interactive with environment $S_t, A_t, R_{t+1}$. Only use those experience to Backup:</p> \[v(S_t) \leftarrow \underset {a\in A} \max \left(R_{S_t}^a + \gamma \underset {s'\in S}\sum P_{S_ts'}^av(s')\right)\] <h3 id="generalized-policy-iteration">Generalized Policy Iteration</h3> <p>Almost all reinforcement learning methods are well described as GPI. That is, all have identiﬁable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram to the right. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function.</p> <h2 id="summary">Summary</h2> <p>Policy evaluation refers to the (typically) iterative computation of the value functions for a given policy.</p> <p>Policy improvement refers to the computation of an improved policy given the value function for that policy.</p> <p>Putting these two computations together, we obtain policy iteration and value iteration, the two most popular DP methods.</p> <p>It is not necessary to perform DP methods in complete sweeps through the state set. Asynchronous DP methods are in-place iterative methods that update states in an arbitrary order.</p>]]></content><author><name></name></author><category term="RL"/><summary type="html"><![CDATA[Reinforcement Learning basic concepts]]></summary></entry><entry><title type="html">Multi-armed Bandits: exploration and exploitation</title><link href="https://ssh022-s.github.io/blog/2020/1mab/" rel="alternate" type="text/html" title="Multi-armed Bandits: exploration and exploitation"/><published>2020-08-25T17:20:00+00:00</published><updated>2020-08-25T17:20:00+00:00</updated><id>https://ssh022-s.github.io/blog/2020/1mab</id><content type="html" xml:base="https://ssh022-s.github.io/blog/2020/1mab/"><![CDATA[<p>You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p> <h3 id="the-multi-armed-bandit">The Multi-Armed Bandit</h3> <p>A multi-armed bandit is a tuple $\langle \cal A, R\rangle$</p> <p>$\cal A$ is a known set of $m$ actions (or “arms”)</p> <p>$\cal R^a(r) = \Bbb P[r|a]$ is an unknown probability distribution over rewards</p> <p>At each step $t$ the agent selects an action $a_t \in \cal A$</p> <p>The environment generates a reward $r_t \sim \cal R^{a_t}$</p> <p>The goal is to maximize cumulative reward $\sum^t_{\tau=1}r_\tau$</p> <hr/> <p>The action-value is the mean reward for action a</p> \[q_*(a) \dot= \Bbb E[R_t|A_t=a]\] <p>We don’t know $q_<em>(a)$. We denote the estimated value of action a at time step t as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_</em>(a)$.</p> <p>action have the greatest estimated value call greedy action. select greedy action call exploiting.</p> <p>greedy action selection method as $A_t \dot=\underset a {\arg\max} Q_t(a)$</p> <p>Greedy action selection always exploits current knowledge to maximize immediate reward.</p> <p>select one of the nongreedy actions are exploring.</p> <p>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</p> <p>we can’t do exploit and explore at the same time.</p> <p>A simple alternative is $\epsilon$-greedy methods. which is behave greedily most of the time, but with small probability $\epsilon$ select randomly from among all the actions with equal probability.</p> <hr/> <h3 id="incremental-implementation">Incremental Implementation</h3> <p>The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. Incremental implementation can computed these averages in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation.</p> <p>Incremental formulas for updating averages with small, constant computation required to process each new reward.</p> \[Q_n \dot=\frac {R_1+R_2+ \cdots + R_{n-1}} {n-1}\] <p>Given $Q_n$ and the $n$th reward, $R_n$, the new average of all n rewards can be computed by</p> \[Q_{n+1} = \frac 1 n \overset n{\underset {i=1} \sum}R_i \\ = Q_n + \frac 1 n [R_n - Q_n]\] <h4 id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h4> <p>We often encounter reinforcement learning problems that are effectively nonstationary. that is, the reward probabilities change over time. In such cases it makes sense to give more weight to recent rewards than to long-past rewards.</p> \[Q_{n+1} \dot= Q_n + \alpha[R_n-Q_n]\] <p>where the step-size parameter $\alpha \in (0, 1]$ is constant.</p> <p><strong>Optimistic Initialization</strong>: Simple and practical idea is initialize Q(a) to high value and update action value by incremental Monte-Carlo evaluation. It will encourages systematic exploration early on.</p> <hr/> <h3 id="upper-conﬁdence-bound-action-selection">Upper-Conﬁdence-Bound Action Selection</h3> <p>The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of $a$’s value.</p> <p>It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions according to</p> \[A_t \dot= \underset a {\arg\max}[Q_t(a)+ c\sqrt{\frac {\ln t} {N_t(a)}}]\] <p>$N_t(a)$ denotes the number of times that action $a$ has been selected prior to time $t$. Each time $a$ is selected the uncertainty is presumably reduced.</p> <hr/> <h3 id="gradient-bandit">Gradient Bandit</h3> <p>Here we consider learning a numerical preference for each action $a$, which we denote $H_t(a)$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important</p> \[Pr\{A_t=a\} \dot= \frac {e^{H_t(a)}} {\sum^k_{b=1}e^{H_t(b)}} \dot=\pi_t(a)\] <p>On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by:</p> \[H_{t+1}(A_t) \dot=H_t(A_t) + \alpha(R_t-\overset -R_t)(1-\pi_t(A_t)), and\\ H_{t+1}(a) \dot=H_t(a) - \alpha(R_t-\overset -R_t)\pi_t(a), for\ all\ a \not= A_t\] <p>If the reward is higher than the baseline $\overset - R_t$, then the probability of taking $A_t$ in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction.</p> <hr/> <h3 id="summary">Summary</h3> <p>Exploration and Exploitaion</p> <ul> <li>Exploration ﬁnds more information about the environment</li> <li>Exploitation exploits known information to maximize reward</li> <li>It is usually important to explore as well as exploit</li> </ul> <p>Several simple ways of balancing exploration and exploitation.</p> <ul> <li>The $\epsilon$-greedy methods choose randomly a small fraction of the time,</li> <li>UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples.</li> <li>Gradient bandit algorithms estimate not action values, but action preferences, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution.</li> <li>The simple expedient of initializing estimates optimistically causes even greedy methods to explore signiﬁcantly.</li> </ul> <p>Reference:</p> <p>Reinforcement Learning An Introduction</p> <p>http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf</p>]]></content><author><name></name></author><category term="RL"/><summary type="html"><![CDATA[Reinforcement Learning basic concepts]]></summary></entry><entry><title type="html">Reinforcement Learning Basic Concepts</title><link href="https://ssh022-s.github.io/blog/2020/0rlbasic/" rel="alternate" type="text/html" title="Reinforcement Learning Basic Concepts"/><published>2020-07-25T17:20:00+00:00</published><updated>2020-07-25T17:20:00+00:00</updated><id>https://ssh022-s.github.io/blog/2020/0rlbasic</id><content type="html" xml:base="https://ssh022-s.github.io/blog/2020/0rlbasic/"><![CDATA[<p>The main characters of RL are the <strong>agent</strong> and the <strong>environment</strong>. The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment changes when the agent acts on it, but may also change on its own.</p> <p>The agent also perceives a <strong>reward</strong> signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward, called <strong>return</strong>. Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal.</p> <p>additional terminology:</p> <ul> <li>states and observations,</li> <li>action spaces,</li> <li>policies,</li> <li>trajectories,</li> <li>different formulations of return,</li> <li>the RL optimization problem,</li> <li>and value functions.</li> </ul> <h3 id="states-and-observations">States and Observations</h3> <p>A <strong>state</strong> is a complete description of the state of the world. There is no information about the world which is hidden from the state. An <strong>observation</strong> is a partial description of a state, which may omit information.</p> <h3 id="action-spaces">Action Spaces</h3> <p>The set of all valid actions in a given environment is often called the <strong>action space</strong>. Some environments, like Atari and Go, have <strong>discrete action spaces</strong>, where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have <strong>continuous action spaces</strong>. In continuous spaces, actions are real-valued vectors.</p> <h3 id="policies">Policies</h3> <p>A <strong>policy</strong> is the agent’s behavior. It is a map from state to action.</p> <p>A <strong>policy</strong> is a rule used by an agent to decide what actions to take.</p> <p>It can be deterministic, in which case it is usually denoted by $\mu$: $a_t = \mu(s_t)$ or it may be stochastic, in which case it is usually denoted by $\pi$: $a_t = \pi(\cdot|s_t)$ or $\pi(a|s) = \Bbb P[A_t=a|S_t=s]$</p> <h4 id="deterministic-policies">Deterministic Policies</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pi_net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">obs_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">(),</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">(),</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">act_dim</span><span class="p">)</span>
            <span class="p">)</span>

<span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">actions</span> <span class="o">=</span> <span class="nf">pi_net</span><span class="p">(</span><span class="n">obs_tensor</span><span class="p">)</span>
</code></pre></div></div> <h4 id="stochastic-policies">Stochastic Policies</h4> <p>The two most common kinds of stochastic policies in deep RL are <strong>categorical policies</strong> and <strong>diagonal Gaussian policies</strong>.</p> <p><strong>Categorical</strong> policies can be used in discrete action spaces, while diagonal <strong>Gaussian</strong> policies are used in continuous action spaces.</p> <p>A categorical policy is like a classifier over discrete actions. one final linear layer that gives you logits for each action, followed by a softmax to convert the logits into probabilities.</p> <p>Two key computations are centrally important for using and training stochastic policies:</p> <ul> <li>sampling actions from the policy,</li> <li>and computing log likelihoods of particular actions, $\log\pi_\theta(a|s)$.</li> </ul> <h3 id="trajectories">Trajectories</h3> <p>A trajectory $\tau$ is a sequence of states and actions in the world,</p> <p>Trajectories are also frequently called <strong>episodes</strong> or <strong>rollouts</strong>.</p> <h3 id="rewards">Rewards</h3> <ul> <li>A <strong>reward</strong> $R_t$ is a scalar feedback signal</li> <li>Indicates how well agent is doing at step $t$</li> <li>The agent’s job is to maximize cumulative reward</li> </ul> <p>Reinforcement learning is based on the <strong>reward hypothesis</strong></p> <p><strong>Definition (Reward Hypothesis):</strong> <em>All</em> goals can be described by the maximization of expected cumulative reward.</p> <p>One kind of return is the <strong>finite-horizon undiscounted return</strong>.</p> <p>Another kind of return is the <strong>infinite-horizon discounted return</strong>, which is the sum of all rewards <em>ever</em> obtained by the agent, but discounted by how far off in the future they’re obtained.</p> <p>Why would we ever want a discount factor, though? Don’t we just want to get <em>all</em> rewards? We do, but the discount factor is both intuitively appealing and mathematically convenient. On an intuitive level: cash now is better than cash later. Mathematically: an infinite-horizon sum of rewards <a href="https://en.wikipedia.org/wiki/Convergent_series">may not converge</a> to a finite value, and is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges.</p> <p>we frequently set up algorithms to optimize the undiscounted return, but use discount factors in estimating <strong>value functions</strong>:</p> <ol> <li>We typically measure the “performance” of an RL algorithm by looking at the average episodic undiscounted sum of rewards, but we implement algorithms that use discount factors in value function learning, and increase probabilities of actions in proportion to advantage functions that use discount factors.</li> <li>From the above, you might get the impression that we’re measuring undiscounted return, but optimizing discounted return. The truth is, we aren’t exactly optimizing for either! If we were truly optimizing for discounted return, there would be a discount factor included in the objective function for each state, corresponding to what timestep in a trajectory that state occurred. But, in typical implementations, we ignore that discount factor and weight each state’s contribution equally.</li> </ol> <hr/> <h3 id="the-rl-problem">The RL Problem</h3> <p>Whatever the choice of return measure (whether infinite-horizon discounted, or finite-horizon undiscounted), and whatever the choice of policy, the goal in RL is to select a policy which maximizes <strong>expected return</strong> when the agent acts according to it.</p> <p>Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a T-step trajectory is: \(P(\tau|\pi) = \rho_0(s_0)\prod_{t=0}^{T-1}P(s_{t+1}|s_t, a_t)\pi(a_t|s_t).\) The expected return (for whichever measure), denoted by $J(\pi)$, is then: \(J(\pi) = \int_\tau P(\tau|\pi)R(\tau) = E_{\tau \sim \pi}[R(\tau)]\) The central optimization problem in RL can then be expressed by \(\pi^* = \arg \max_\pi J(\pi),\) with $\pi^*$ being the <strong>optimal policy</strong></p> <hr/> <p>An RL agent may include one or more of these components:</p> <ul> <li>Policy: agent’s behavior function</li> <li>Value function: how good is each state and/or action</li> <li>Model: agent’s representation of the environment</li> </ul> <h3 id="value-functions">Value Functions</h3> <ul> <li> <p>Value function is a prediction of future reward</p> </li> <li> <p>Used to evaluate the goodness/badness of states</p> </li> <li> <p>And therefore to select between actions, e.g. \(V_\pi(s) = \Bbb E_\pi[R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + ...|S_t=s]\)</p> </li> </ul> <p>There are four main functions of note here.</p> <ol> <li>The <strong>On-Policy Value Function</strong>, $V^\pi(s) = E_{\tau\sim\pi}[R(\tau)|s_0=s]$</li> <li>The <strong>On-Policy Action-Value Function</strong>, $Q^\pi(s, a) = E_{\tau\sim\pi}[R(\tau)|s_0=s, a_0=a]$</li> <li>The <strong>Optimal Value Function</strong>, $V^*(s) = \max_\pi E_{\tau\sim\pi}[R(\tau)|s_0=s]$</li> <li>The <strong>Optimal Action-Value Function</strong>, $Q^*(s, a) = \max_\pi E_{\tau\sim\pi}[R(\tau)|s_0=s, a_0=a]$</li> </ol> <p>two key connections between the value function and the action-value function: \(V^\pi(s) = E_{a\sim\pi}[Q^\pi(s, a)], \\ V^*(s) = \max_a Q^*(s, a).\) The optimal action: \(a^*(s) = \arg \max_aQ^*(s, a).\)</p> <p><strong>Model</strong></p> <ul> <li> <p>A <strong>model</strong> predicts what the environment will do next</p> </li> <li> <p>$P$ predicts the next state</p> </li> <li> <p>$R$ predicts the next (immediate) reward, e.g. \(P_{ss'}^a = \Bbb P[S_{t+1}=s'|S_t=s, A_t=a] \\ R_s^a = \Bbb E[R_{t+1}|S_t=s, A_t = a]\)</p> </li> </ul> <hr/> <h3 id="bellman-equations">Bellman Equations</h3> <p>The basic idea behind the Bellman equations is this:</p> <blockquote> <p>The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.</p> </blockquote> <p>The Bellman equations for the on-policy value functions are \(V^\pi(s) = \underset {s'\sim P} {\underset {a\sim\pi} E}[r(s, a)+\gamma V^\pi(s')], \\ Q^\pi(s, a) = \underset {s'\sim P} E[r(s, a) + \gamma \underset {a'\sim\pi}E[Q^\pi(s', a')]]\)</p> <h3 id="advantage-functions">Advantage Functions</h3> <p>Sometimes in RL, we don’t need to describe how good an action is in an absolute sense, but only how much better it is than others on average. That is to say, we want to know the relative <strong>advantage</strong> of that action. We make this concept precise with the <strong>advantage function.</strong> \(A^\pi(s, a) = Q^\pi(s, a)-V^\pi(s).\)</p> <hr/> <hr/> <h2 id="formalism">Formalism</h2> <p>So far, we’ve discussed the agent’s environment in an informal way, but if you try to go digging through the literature, you’re likely to run into the standard mathematical formalism for this setting: <strong>Markov Decision Processes</strong> (MDPs). An MDP is a 5-tuple, $\langle S, A, R, P, \rho_0 \rangle$.</p> <p>The name Markov Decision Process refers to the fact that the system obeys the <a href="https://en.wikipedia.org/wiki/Markov_property">Markov property</a>: transitions only depend on the most recent state and action, and no prior history.</p> <h3 id="history-and-state">History and state</h3> <p>The <strong>history</strong> is the sequence of observations, actions, rewards \(H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t\) <strong>State</strong> is the information used to determine what happens next.</p> <p>state is a function of the history: \(S_t = f(H_t)\)</p> <h3 id="information-state">Information State</h3> <p>An <strong>information state</strong> (a.k.a <strong>Markov state</strong>) contains all useful information from the history.</p> <p>A state $S_t$ is <strong>Markov</strong> if and only if \(\Bbb P[S_{t+1}|S_t] = \Bbb P[S_{t+1}|S_1, ..., S_t]\)</p> <ul> <li> <p>The future is independent of the past given the present \(H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infin}\)</p> </li> <li> <p>Once the state is known, the history may be thrown away</p> </li> <li> <p>i.e. The state is a sufficient statistic of the future</p> </li> <li> <p>The environment state $S_t^e$ is Markov</p> </li> <li> <p>The history $H_t$ is Markov</p> </li> </ul> <hr/> <p><em>Markov decision processes</em> formally describe an environment for reinforcement learning</p> <p>Where the environment is <em>fully observable</em></p> <p><strong>Full observability:</strong> agent directly observes environment state \(O_t = S_t^a = S_t^e\)</p> <ul> <li>Agent state = environment state = information state</li> <li>Formally, this is a <strong>Markov decision process</strong> (MDP)</li> </ul> <p><strong>Partial observability:</strong> agent indirectly observes environment.</p> <ul> <li>Now agent state $\not=$ environment state</li> <li>Formally this is a <strong>partially observable Markov decision process</strong> (POMDP)</li> </ul> <p>Definition $G_t$​</p> <p>The <em>return</em> $G_t$ is the total discounted reward from time-step $t$. \(G_t = R_{t+1} + \gamma R_{t+2} +\ ...=\underset {k=0} {\overset \infty \sum} \gamma^kR_{t+k+1}\)</p> <ul> <li>The <em>discount</em> $\gamma \in [0, 1]$ is the present value of future rewards</li> <li>The value of receiving reward R after k+1 time-steps is $\gamma^kR$</li> <li>This values immediate reward above delayed reward.</li> <li>$\gamma$ close to 0 leads to “myopic” evaluation</li> <li>$\gamma$ close to 1 leads to “far-sighted” evaluation</li> </ul> <p>The value function $v(s)$ gives the long-term value of state $s$</p> <p>Definition <strong>state value function</strong></p> <p>The <em>state value function</em> v(s) of an MRP is the expected return starting from state $s$ \(v(s) = \Bbb E[G_t|S_t=s]\) <strong>Bellman Equation for MRPs</strong></p> <p>The value function can be decomposed into two parts:</p> <ul> <li>immediate reward $R_{t+1}$</li> <li>discounted value of successor state $\gamma v(S_{t+1})$</li> </ul> \[v(s) = \Bbb E[G_t|S_t=s] = \Bbb E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]\] <hr/> <p><strong>Markov Decision Process</strong></p> <p>A Markov decision process (MDP) is a Markov reward process with decisions. It is an <em>environment</em> in which all states are Markov.</p> <p><strong>Definition</strong></p> <p>A <em>Markov Decision Process</em> is a tuple $&lt;S, A, P, R, \gamma&gt;$</p> <ul> <li> <p>$S$ is a finite set of states</p> </li> <li> <p>A is a finite set of actions</p> </li> <li> <p>P is a state transition probability matrix, $P_{ss’}^a=\Bbb P[S_{t+1}=s’|S_t=s, A_t=a]$</p> </li> <li> <p>R is a reward function, $R_s^a = \Bbb E[R_{t+1}|S_t=s, A_t=a]$</p> </li> <li> <p>$\gamma$ is a discount factor $\gamma \in [0, 1]$</p> </li> </ul> <p><strong>Policies</strong> for MDP</p> <p><strong>Definition</strong></p> <p>A <em>policy</em> $\pi$ is a distribution over actions given states, \(\pi(a|s) = \Bbb P[A_t=a|S_t=s]\)</p> <ul> <li>A policy fully defines the behavior of an agent</li> <li>MDP policies depend on the current state (not the history)</li> </ul> <p><strong>Value Function</strong> for MDP</p> <p><strong>Definition</strong></p> <p>The <em>state-value</em> function $v_\pi(s)$ of an MDP is the expected return starting from state s, and then following policy $\pi$ \(v_\pi(s) = \Bbb E_\pi[G_t|S_t=s]\) The <em>action-value</em> function $q_\pi(s, a)$ is the expected return starting from state s, taking action a, and then following policy $\pi$ \(q_\pi(s, a)=\Bbb E_\pi[G_t|S_t=s, A_t=a]\)</p> <hr/> <p>Bellman Expectation Equation</p> <p>The state-value function can again be decomposed into immediate reward plus discounted value of successor state, \(v_\pi(s) = \Bbb E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\) The action-value function can similarly be decomposed, \(q_\pi(s, a)=\Bbb E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1}, A_{t+1})|S_t=s, A_t=a]\)</p> <hr/> <h3 id="formula-derivation">Formula derivation:</h3> <p>Expected discounted return: \(\begin{align} G_t &amp; \dot= R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \gamma^3R_{t+4}+ \dots \\ &amp; = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2R_{t+4}+ \dots) \\ &amp; = R_{t+1} + \gamma G_{t+1} \end{align}\) where $\gamma$ is a parameter, $0 \le \gamma \le 1$, called the discount rate.</p> <p>Bellman equation for $v_\pi$ \(\begin{align} v_\pi(s) &amp;\dot=\Bbb E_\pi[G_t|S_t=s]\\ &amp;= \Bbb E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s]\\ &amp;= \underset a \sum\pi(a|s)\underset {s'} \sum\underset r \sum p(s', r|s,a)[r+\gamma \Bbb E_\pi[G_{t+1}|S_{t+1}=s']]\\ &amp;=\underset a \sum \pi(a|s) \underset {s',r} \sum p(s', r|s,a)[r+\gamma v_\pi(s')],\ for\ all\ s \in \cal S \end{align}\)</p> <hr/> <p>We denote all the <em>optimal policies</em> by $\pi_*$.</p> <p>$\pi \ge \pi’$ if and only if $v_\pi(s) \ge v_{\pi’}(s)$ for all $s \in \cal S$</p> <p><em>optimal state-value function</em></p> <p>$v_*(s) \dot= \underset \pi \max v_\pi(s)$, for all $s \in \cal S$</p> <p><em>optimal action-value function</em>, denoted $q_*$:</p> <p>$q_*(s, a) \dot=\underset \pi \max q_\pi(s, a)$, for all $s \in \cal S$ and $a \in \cal A(s)$.</p> <p>$q_<em>$ in terms of $v_</em>$:</p> \[q_*(s, a) = \Bbb E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a]\] <hr/> <p>Bellman optimality equation for $v_*$</p> \[\begin{align} v_\pi(s) &amp;= \underset {a \in \cal A(s)} \max q_{\pi_*}(s, a)\\ &amp;= \underset a \max \Bbb E_{\pi_*}[G_t|S_t=s, A_t=a]\\ &amp;= \underset a \max \Bbb E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s, A_t=a]\\ &amp;= \underset a \max \Bbb E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a] \\ &amp;=\underset a \max\underset {s',r} \sum p(s', r|s,a)[r+\gamma v_\pi(s')] \end{align}\] <p>Bellman optimality equation for $q_*$</p> \[q_*(s, a) = \Bbb E[R_{t+1} + \gamma \underset {a'} \max q_*(S_{t+1}, a') | S_t=s, A_t=a]\\ = \underset {s', r} \sum p(s', r|s, a)[r+ \gamma \underset {a'} \max q_*(s', a')].\] <p>Reference:</p> <p>Reinforcement Learning An Introduction</p>]]></content><author><name></name></author><category term="RL"/><summary type="html"><![CDATA[Reinforcement Learning basic concepts and Finite Markov Decision Processes]]></summary></entry></feed>