<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Dynamic Programming | Shihao Sun </title> <meta name="author" content="Shihao Sun"> <meta name="description" content="Reinforcement Learning basic concepts"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ssh022-s.github.io/blog/2020/2dp/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shihao</span> Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Dynamic Programming</h1> <p class="post-meta"> Created in September 09, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/rl"> <i class="fa-solid fa-tag fa-sm"></i> RL</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>What is dynamic programming?</p> <p>The term dynamic programming (DP) refers to a collection of algorithms that can be used to <strong>compute optimal policies</strong> given a <strong>perfect model of the environment as a Markov decision process (MDP)</strong>.</p> <p><strong>Dynamic</strong>: sequential or temporal component to the problem</p> <p><strong>Programming</strong>: optimizing a “program”, i.e. a policy</p> <p>Dynamic programming assumes full knowledge of the MDP</p> <p>Dynamic Programming is a very general solution method for problems which have two properties:</p> <ul> <li>Optimal substructure <ul> <li>Principle of optimality applies</li> <li>Optimal solution can be decomposed into subproblems</li> </ul> </li> <li>Overlapping subproblems <ul> <li>Subproblems recur many times</li> <li>Solutions can be cached and reused</li> </ul> </li> </ul> <p>Markov decision processes satisfy both properties</p> <ul> <li>Bellman equation gives recursive decomposition</li> <li>Value function stores and reuses solutions</li> </ul> <h3 id="policy-evaluation-prediction">Policy Evaluation (Prediction)</h3> <p>We consider how to compute the state-value function $v_\pi$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem.</p> \[v_\pi(s) \dot = \underset a \sum \pi(a|s) \underset {s',r} \sum p(s',r|s,a)[r+\gamma v_\pi(s')]\] <p>where $\pi(a|s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$, and the expectations are subscripted by $\pi$ to indicate that they are conditional on $\pi$ being followed.</p> \[\begin{align} v_{k+1}(s)\ &amp; \dot=\ \Bbb E_\pi[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s]\\ &amp; = \underset a \sum \pi(a|s)\underset {s',r}\sum p(s',r|s,a)[r+\gamma v_k(s')] \end{align}\] <p>The sequence ${v_k}$ can be shown in general to converge to $v_\pi$ as $k\rightarrow \infty$ under the same conditions that guarantee the existence of $v_\pi$.</p> <h3 id="policy-improvement">Policy Improvement</h3> <p>Consider a deterministic policy, $a = \pi(s)$</p> <p>We can <em>improve</em> the policy by acting greedily</p> \[\pi'(s) = \underset {a \in A} {\arg \max}\ q_\pi(s, a)\] <p>Let $\pi$ and $\pi’$ be any pair of deterministic policies such that, for all $s\in S$,</p> \[q_\pi(s, \pi'(s)) \geq v_\pi(s)\] <p>Then the policy $\pi’$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states $s\in S$:</p> \[v_{\pi'} \geq v_\pi(s)\] <p>new <em>greedy</em> policy, $\pi’$, given by</p> \[\begin{align} \pi'(s)\ &amp; \dot =\ \underset a {\arg \max}\ q_{\pi}(s, a)\\ &amp; = \underset a {\arg \max}\ \Bbb E[R_{t+1}+\gamma v_{\pi}(S_{t+1})\ |\ S_t=s, A_t=a]\\ &amp; = \underset a {\arg \max}\ \underset {s',r} \sum p(s',r|s,a)[r+\gamma v_\pi(s')] \end{align}\] <h3 id="value-iteration">Value Iteration</h3> <p>If we know the solution to subproblems $v_<em>(s’)$, then solution $v_</em>(s)$ can be found by one-step lookahead</p> \[v_*(s) \leftarrow \underset {a\in A} \max R_s^a + \gamma \underset {s'\in S}\sum P_{ss'}^av_*(s')\] <p>Value iteration is obtained simply by turning the Bellman optimality equation into an update rule.</p> <p>Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement.</p> <h3 id="asynchronous-dynamic-programming">Asynchronous Dynamic Programming</h3> <p>Three simple ideas for asynchronous dynamic programming:</p> <ul> <li>In-place dynamic programming</li> <li>Prioritized sweeping</li> <li>Real-time dynamic programming</li> </ul> <p><strong>In-place dynamic programming</strong></p> <p>In-place value iteration only stores one copy of value function. that is, with each new value immediately overwriting the old one. It usually converges faster than the two-copies version, because it uses new data as soon as they are available.</p> <p><strong>Prioritized Sweeping</strong></p> <p>Use magnitude of Bellman error to guide state selection. Backup the state with the largest remaining Bellman error. Can be implemented eﬃciently by maintaining a priority queue.</p> <p><strong>Real-time dynamic programming</strong></p> <p>Idea: only states that are relevant to agent.</p> <p>After each time-step agent interactive with environment $S_t, A_t, R_{t+1}$. Only use those experience to Backup:</p> \[v(S_t) \leftarrow \underset {a\in A} \max \left(R_{S_t}^a + \gamma \underset {s'\in S}\sum P_{S_ts'}^av(s')\right)\] <h3 id="generalized-policy-iteration">Generalized Policy Iteration</h3> <p>Almost all reinforcement learning methods are well described as GPI. That is, all have identiﬁable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram to the right. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function.</p> <h2 id="summary">Summary</h2> <p>Policy evaluation refers to the (typically) iterative computation of the value functions for a given policy.</p> <p>Policy improvement refers to the computation of an improved policy given the value function for that policy.</p> <p>Putting these two computations together, we obtain policy iteration and value iteration, the two most popular DP methods.</p> <p>It is not necessary to perform DP methods in complete sweeps through the state set. Asynchronous DP methods are in-place iterative methods that update states in an arbitrary order.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/7dqn/">DQN</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/6pga/">Policy Gradient Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/5pg/">Policy Gradient Method</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/4fa/">Function Approximation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/3mfp/">Model-Free Prediction</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shihao Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-dqn",title:"DQN",description:"DQN and it's improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2020/7dqn/"}},{id:"post-policy-gradient-algorithms",title:"Policy Gradient Algorithms",description:"Introduce Policy Gradient Algorithms includes VPG, PPO, A3C, DDPG, TD3, SAC.",section:"Posts",handler:()=>{window.location.href="/blog/2020/6pga/"}},{id:"post-policy-gradient-method",title:"Policy Gradient Method",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/5pg/"}},{id:"post-function-approximation",title:"Function Approximation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/4fa/"}},{id:"post-model-free-prediction",title:"Model-Free Prediction",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/3mfp/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/2dp/"}},{id:"post-multi-armed-bandits-exploration-and-exploitation",title:"Multi-armed Bandits: exploration and exploitation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/1mab/"}},{id:"post-reinforcement-learning-basic-concepts",title:"Reinforcement Learning Basic Concepts",description:"Reinforcement Learning basic concepts and Finite Markov Decision Processes",section:"Posts",handler:()=>{window.location.href="/blog/2020/0rlbasic/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-beast-engine",title:"Beast Engine",description:"AI-Native Game Engine",section:"Projects",handler:()=>{window.location.href="/projects/Beast/"}},{id:"projects-machine-learning-for-blast-performance-analysis-in-mining",title:"Machine Learning for Blast Performance Analysis in Mining",description:"Automated Blast Rating from Drone Footage",section:"Projects",handler:()=>{window.location.href="/projects/Blasting/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%32%32%39%35%35%32%39%34%38%32@%71%71.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/liushuai26","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@aicat-","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>