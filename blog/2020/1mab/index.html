<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multi-armed Bandits: exploration and exploitation | Shuai Liu </title> <meta name="author" content="Shuai Liu"> <meta name="description" content="Reinforcement Learning basic concepts"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liushuai26.github.io/blog/2020/1mab/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shuai</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Multi-armed Bandits: exploration and exploitation</h1> <p class="post-meta"> Created in August 25, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/rl"> <i class="fa-solid fa-tag fa-sm"></i> RL</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>You are faced repeatedly with a choice among k different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p> <h3 id="the-multi-armed-bandit">The Multi-Armed Bandit</h3> <p>A multi-armed bandit is a tuple $\langle \cal A, R\rangle$</p> <p>$\cal A$ is a known set of $m$ actions (or “arms”)</p> <p>$\cal R^a(r) = \Bbb P[r|a]$ is an unknown probability distribution over rewards</p> <p>At each step $t$ the agent selects an action $a_t \in \cal A$</p> <p>The environment generates a reward $r_t \sim \cal R^{a_t}$</p> <p>The goal is to maximize cumulative reward $\sum^t_{\tau=1}r_\tau$</p> <hr> <p>The action-value is the mean reward for action a</p> \[q_*(a) \dot= \Bbb E[R_t|A_t=a]\] <p>We don’t know $q_<em>(a)$. We denote the estimated value of action a at time step t as $Q_t(a)$. We would like $Q_t(a)$ to be close to $q_</em>(a)$.</p> <p>action have the greatest estimated value call greedy action. select greedy action call exploiting.</p> <p>greedy action selection method as $A_t \dot=\underset a {\arg\max} Q_t(a)$</p> <p>Greedy action selection always exploits current knowledge to maximize immediate reward.</p> <p>select one of the nongreedy actions are exploring.</p> <p>Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.</p> <p>we can’t do exploit and explore at the same time.</p> <p>A simple alternative is $\epsilon$-greedy methods. which is behave greedily most of the time, but with small probability $\epsilon$ select randomly from among all the actions with equal probability.</p> <hr> <h3 id="incremental-implementation">Incremental Implementation</h3> <p>The action-value methods we have discussed so far all estimate action values as sample averages of observed rewards. Incremental implementation can computed these averages in a computationally efficient manner, in particular, with constant memory and constant per-time-step computation.</p> <p>Incremental formulas for updating averages with small, constant computation required to process each new reward.</p> \[Q_n \dot=\frac {R_1+R_2+ \cdots + R_{n-1}} {n-1}\] <p>Given $Q_n$ and the $n$th reward, $R_n$, the new average of all n rewards can be computed by</p> \[Q_{n+1} = \frac 1 n \overset n{\underset {i=1} \sum}R_i \\ = Q_n + \frac 1 n [R_n - Q_n]\] <h4 id="tracking-a-nonstationary-problem">Tracking a Nonstationary Problem</h4> <p>We often encounter reinforcement learning problems that are effectively nonstationary. that is, the reward probabilities change over time. In such cases it makes sense to give more weight to recent rewards than to long-past rewards.</p> \[Q_{n+1} \dot= Q_n + \alpha[R_n-Q_n]\] <p>where the step-size parameter $\alpha \in (0, 1]$ is constant.</p> <p><strong>Optimistic Initialization</strong>: Simple and practical idea is initialize Q(a) to high value and update action value by incremental Monte-Carlo evaluation. It will encourages systematic exploration early on.</p> <hr> <h3 id="upper-conﬁdence-bound-action-selection">Upper-Conﬁdence-Bound Action Selection</h3> <p>The idea of this upper conﬁdence bound (UCB) action selection is that the square-root term is a measure of the uncertainty or variance in the estimate of $a$’s value.</p> <p>It would be better to select among the non-greedy actions according to their potential for actually being optimal, taking into account both how close their estimates are to being maximal and the uncertainties in those estimates. One effective way of doing this is to select actions according to</p> \[A_t \dot= \underset a {\arg\max}[Q_t(a)+ c\sqrt{\frac {\ln t} {N_t(a)}}]\] <p>$N_t(a)$ denotes the number of times that action $a$ has been selected prior to time $t$. Each time $a$ is selected the uncertainty is presumably reduced.</p> <hr> <h3 id="gradient-bandit">Gradient Bandit</h3> <p>Here we consider learning a numerical preference for each action $a$, which we denote $H_t(a)$. The larger the preference, the more often that action is taken, but the preference has no interpretation in terms of reward. Only the relative preference of one action over another is important</p> \[Pr\{A_t=a\} \dot= \frac {e^{H_t(a)}} {\sum^k_{b=1}e^{H_t(b)}} \dot=\pi_t(a)\] <p>On each step, after selecting action $A_t$ and receiving the reward $R_t$, the action preferences are updated by:</p> \[H_{t+1}(A_t) \dot=H_t(A_t) + \alpha(R_t-\overset -R_t)(1-\pi_t(A_t)), and\\ H_{t+1}(a) \dot=H_t(a) - \alpha(R_t-\overset -R_t)\pi_t(a), for\ all\ a \not= A_t\] <p>If the reward is higher than the baseline $\overset - R_t$, then the probability of taking $A_t$ in the future is increased, and if the reward is below baseline, then the probability is decreased. The non-selected actions move in the opposite direction.</p> <hr> <h3 id="summary">Summary</h3> <p>Exploration and Exploitaion</p> <ul> <li>Exploration ﬁnds more information about the environment</li> <li>Exploitation exploits known information to maximize reward</li> <li>It is usually important to explore as well as exploit</li> </ul> <p>Several simple ways of balancing exploration and exploitation.</p> <ul> <li>The $\epsilon$-greedy methods choose randomly a small fraction of the time,</li> <li>UCB methods choose deterministically but achieve exploration by subtly favoring at each step the actions that have so far received fewer samples.</li> <li>Gradient bandit algorithms estimate not action values, but action preferences, and favor the more preferred actions in a graded, probabilistic manner using a soft-max distribution.</li> <li>The simple expedient of initializing estimates optimistically causes even greedy methods to explore signiﬁcantly.</li> </ul> <p>Reference:</p> <p>Reinforcement Learning An Introduction</p> <p>http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/XX.pdf</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/7dqn/">DQN</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/6pga/">Policy Gradient Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/5pg/">Policy Gradient Method</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/4fa/">Function Approximation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/3mfp/">Model-Free Prediction</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shuai Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-dqn",title:"DQN",description:"DQN and it's improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2020/7dqn/"}},{id:"post-policy-gradient-algorithms",title:"Policy Gradient Algorithms",description:"Introduce Policy Gradient Algorithms includes VPG, PPO, A3C, DDPG, TD3, SAC.",section:"Posts",handler:()=>{window.location.href="/blog/2020/6pga/"}},{id:"post-policy-gradient-method",title:"Policy Gradient Method",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/5pg/"}},{id:"post-function-approximation",title:"Function Approximation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/4fa/"}},{id:"post-model-free-prediction",title:"Model-Free Prediction",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/3mfp/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/2dp/"}},{id:"post-multi-armed-bandits-exploration-and-exploitation",title:"Multi-armed Bandits: exploration and exploitation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/1mab/"}},{id:"post-reinforcement-learning-basic-concepts",title:"Reinforcement Learning Basic Concepts",description:"Reinforcement Learning basic concepts and Finite Markov Decision Processes",section:"Posts",handler:()=>{window.location.href="/blog/2020/0rlbasic/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-beast-engine",title:"Beast Engine",description:"AI-Native Game Engine",section:"Projects",handler:()=>{window.location.href="/projects/Beast/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%68%75%61%69.%6C%69%75.%61%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/liushuai26","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@aicat-","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>