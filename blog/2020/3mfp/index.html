<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Model-Free Prediction | Shihao Sun </title> <meta name="author" content="Shihao Sun"> <meta name="description" content="Reinforcement Learning basic concepts"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ssh022-s.github.io/blog/2020/3mfp/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shihao</span> Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Model-Free Prediction</h1> <p class="post-meta"> Created in October 25, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/rl"> <i class="fa-solid fa-tag fa-sm"></i> RL</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Estimate the value function of an unknown MDP.</p> <ul> <li>Monte Carlo Prediction</li> <li>Temporal-Difference (TD) Prediction</li> </ul> <hr> <p>Monte Carlo Methods</p> <p>Monte Carlo methods require only experience—sample sequences of states, actions, and rewards from actual or simulated interaction with an environment.</p> <ul> <li> <p>MC methods learn directly from episodes of experience</p> </li> <li> <p>MC is model-free: no knowledge of MDP transitions / rewards</p> </li> <li> <p>MC learns from complete episodes: no bootstrapping</p> </li> <li> <p>MC uses the simplest possible idea: value = mean return</p> </li> </ul> <p>Here we deﬁne Monte Carlo methods only for episodic tasks. That is, we assume experience is divided into episodes, and that all episodes eventually terminate no matter what actions are selected.</p> <p>TD methods</p> <ul> <li> <p>TD methods learn directly from episodes of experience</p> </li> <li> <p>TD is model-free: no knowledge of MDP transitions / rewards</p> </li> <li> <p>TD learns from incomplete episodes, by bootstrapping</p> </li> <li> <p>TD updates a guess towards a guess</p> </li> </ul> <hr> <h3 id="monte-carlo-prediction">Monte Carlo Prediction</h3> <p>An obvious way to estimate it from experience, then, is simply to average the returns observed after visits to that state. As more returns are observed, the average should converge to the expected value.</p> <p>Goal: learn $v_\pi$ from episodes of experience under policy $\pi$</p> <p>return is the total discounted reward:</p> <p>$G_t = R_{t+1} + \gamma R_{t+2} + … + \gamma^{T-1}R_T$</p> <p>Each average is itself an unbiased estimate, and the standard deviation of its error falls as $1/\sqrt n$, where $n$ is the number of returns averaged. Whereas the DP diagram shows all possible transitions, the Monte Carlo diagram shows only those sampled on the one episode. Monte Carlo methods do not bootstrap. One can generate many sample episodes starting from the states of interest, averaging returns from only these states, ignoring all others.</p> <h3 id="td-prediction">TD Prediction</h3> <p>Firstly, recall the monte carlo method. Monte Carlo methods wait until the return following the visit is known, then use that return as a target for $V(S_t)$. A simple every-visit Monte Carlo method suitable for nonstationary environments is</p> \[V(S_t) \leftarrow V(S_t) + \alpha[G_t-V(S_t)]\] <p>Whereas Monte Carlo methods must wait until the end of the episode to determine the increment to $V(St)$ (only then is $G_t$ known), TD methods need to wait only until the next time step. At time $t + 1$ they immediately form a target and make a useful update using the observed reward $R_{t+1}$ and the estimate $V(S_{t+1})$. The simplest TD method makes the update:</p> \[V(S_t) \leftarrow V(S_t) + \alpha[R_{t+1}+\gamma V(S_{t+1})-V(S_t)]\] <p>The target for the TD update is <strong>$R_{t+1}+\gamma V(S_{t+1})$</strong></p> <p>Measuring the difference between <strong>the estimated value of $S_t$</strong> and <strong>the better estimate $R_{t+1}+\gamma V(S_{t+1})$</strong>. This quantity, called the $TD\ error$:</p> \[\delta_t\ \dot =\ R_{t+1} + \gamma V(S_{t+1})-V(S_t)\] <h4 id="mc-and-td">MC and TD</h4> <p>Goal: learn $v_\pi$ online from experience under policy $\pi$</p> <p>Incremental every-visit Monte-Carlo</p> <ul> <li> <p>update value $V(S_t)$ toward <em>actual</em> return $G_t$</p> \[V(S_t) \leftarrow V(S_t) + \alpha (G_t-V(S_t))\] </li> </ul> <p>Simplest temporal-difference learning algorithm: TD(0)</p> <ul> <li> <p>Update value $V(S_t)$ toward <em>estimated</em> return $R_{t+1}+\gamma V(S_{t+1})$</p> \[V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1}+\gamma V(S_{t+1})- V(S_t))\] </li> </ul> <h4 id="advantages-and-disadvantages-of-mc-vs-td">Advantages and Disadvantages of MC vs. TD</h4> <p>TD can learn before knowing the final outcome</p> <ul> <li>TD can learn online after every step</li> <li>MC must wait until end of episode before return is known</li> </ul> <p>TD can learn without the final outcome</p> <ul> <li>TD can learn from incomplete sequences</li> <li>MC can only learn from complete sequences</li> <li>TD works in continuing (non-terminating) environments</li> <li>MC only works for episodic (terminating) environments</li> </ul> <p>TD methods have usually been found to converge faster than constant-$\alpha$ MC methods on stochastic tasks</p> <hr> <h2 id="model-free-control">Model-Free Control</h2> <p>Policy improvement.</p> <ul> <li>Monte-Carlo Control</li> <li>TD Control</li> </ul> <p>Model-Free Policy Iteration Using Action-Value Function.</p> <p>Greedy policy improvement over V(s) requires model of MDP</p> \[\pi'(s) = \underset{a \in A} {\arg\max} \cal R^a_s+\cal P^a_{ss'}V(s')\] <p>Policy improvement is done by making the policy greedy with respect to the current value function. In this case we have an action-value function, and therefore no model is needed to construct the greedy policy.</p> <p>Greedy policy improvement over $Q(s, a)$ is model-free</p> \[\pi'(s) = \underset {a \in A} {\arg \max} Q(s, a)\] <p>problem about greedy action selection: never exploration.</p> <h4 id="epsilon-greedy-exploration">$\epsilon$-Greedy Exploration</h4> <p>$\epsilon$-Greedy, Simplest idea for ensuring continual exploration. meaning that most of the time they choose an action that has maximal estimated action value, but with probability $\epsilon$ they instead select an action at random.</p> <h3 id="monte-carlo-control">Monte-Carlo Control</h3> <h3 id="td-control">TD Control</h3> <p>Temporal-difference (TD) learning has several advantages over Monte-Carlo (MC)</p> <ul> <li>Lower variance</li> <li>Online</li> <li>Incomplete sequences</li> </ul> <p>Natural idea: use TD instead of MC in our control loop</p> <ul> <li>Apply TD to Q(S,A)</li> <li>Use $\epsilon$-greedy policy improvement</li> <li><strong>Update every time-step</strong></li> </ul> <h3 id="sarsa-on-policy-td-control">Sarsa: On-policy TD Control</h3> \[Q(S, A) \leftarrow Q(S, A) + \alpha (R+\gamma Q(S', A')-Q(S,A))\] <p>This update is done after every transition from a nonterminal state St. If $S’$ is terminal, then $Q(S’, A’)$ is defined as zero.</p> <p>n-step Sarsa</p> <p>n = 1 (Sarsa) $q_t^{(1)} = R_{t+1} + \gamma Q(S_{t+1})$</p> <p>n = 2 $q_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 Q(S_{t+2})$</p> <p>…</p> <p>n = $\infty$ (MC) $q_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} +\ …\ + \gamma^{T-1}R_T$</p> <p>n-step Q-return $q_t^{(n)} = R_{t+1} + \gamma R_{t+2} +\ …\ + \gamma^{n-1}R_{t+n}+\gamma^nQ(S_{t+n})$</p> <p>n-step Sarsa updates Q(s,a) towards the n-step Q-return:</p> \[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(q_t^{(n)}-Q(S_t, A_t))\] <p>**Sarsa(λ) Algorithm: **</p> <h4 id="off-policy-learning">Off-Policy Learning</h4> <p>Evaluate target policy $\pi(a|s)$ to compute $v_\pi(s)$ or $q_\pi(s, a)$</p> <p>While following behavior policy $\mu(a|s)$</p> \[\{S_1,A_1,R_2,\ ...,S_T\}\sim \mu\] <p>Re-use experience generated from old policies $\pi_1, \pi_2, \ …, \pi_{t-1}$, Learn from observing humans or other agents.</p> <p>Learn about optimal policy while following exploratory policy</p> <p>Learn about multiple policies while following one policy</p> <h4 id="q-learning-off-policy-td-control">Q-Learning: Off-policy TD Control</h4> <p>consider oﬀ-policy learning of action-values $Q(s, a)$</p> <p>Next action is chosen using behavior policy $A_{t+1} \sim \mu(\cdot|S_t)$</p> <p>But we consider alternative successor action $A’ \sim \pi(\cdot|S_t)$</p> <p>And update $Q(S_t, A_t)$ towards value of alternative action</p> \[Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha(R_{t+1}+\gamma Q(S_{t+1}, A')-Q(S_t, A_t))\] <p>The target policy $\pi$ is greedy w.r.t $Q(s, a)$</p> \[\pi(S_{t+1}) = \underset {a'} {\arg\max} Q(S_{t+1}, a')\] <p>The behavior policy $\mu$ is e.g. $\epsilon$-greedy w.r.t $Q(s, a)$</p> \[\begin{align} &amp;\ \ \ \ \ R_{t+1} + \gamma Q(S_{t+1}, A') \\ &amp; = R_{t+1} + \gamma Q(S_{t+1}, \underset {a'} {\arg\max}Q(S_{t+1}, a')) \\ &amp; = R_{t+1} + \underset {a'} \max \gamma Q(S_{t+1}, a') \end{align}\] <p>Q-Learning Control Algorithm</p> \[Q(S, A) \leftarrow Q(S, A) + \alpha(R+\gamma \underset {a'} \max Q(S', a')-Q(S, A))\] <p>Sarsa (State-Action-Reward-State-Action): The update is based on the action actually taken by the agent. \(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( R_{t+1} + \gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t) \right)\) Sarsa uses the next action a_{t+1} chosen by the current policy, making it an on-policy algorithm. Q-learning: The update is based on the maximum possible action-value from the next state, regardless of the action actually taken. \(Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha \left( R_{t+1} + \gamma \max_{a} Q(s_{t+1}, a) - Q(s_t, a_t) \right)\) Q-learning uses the maximum possible reward from the next state, making it an off-policy algorithm.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/7dqn/">DQN</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/6pga/">Policy Gradient Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/5pg/">Policy Gradient Method</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/4fa/">Function Approximation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/2dp/">Dynamic Programming</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shihao Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-dqn",title:"DQN",description:"DQN and it's improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2020/7dqn/"}},{id:"post-policy-gradient-algorithms",title:"Policy Gradient Algorithms",description:"Introduce Policy Gradient Algorithms includes VPG, PPO, A3C, DDPG, TD3, SAC.",section:"Posts",handler:()=>{window.location.href="/blog/2020/6pga/"}},{id:"post-policy-gradient-method",title:"Policy Gradient Method",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/5pg/"}},{id:"post-function-approximation",title:"Function Approximation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/4fa/"}},{id:"post-model-free-prediction",title:"Model-Free Prediction",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/3mfp/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/2dp/"}},{id:"post-multi-armed-bandits-exploration-and-exploitation",title:"Multi-armed Bandits: exploration and exploitation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/1mab/"}},{id:"post-reinforcement-learning-basic-concepts",title:"Reinforcement Learning Basic Concepts",description:"Reinforcement Learning basic concepts and Finite Markov Decision Processes",section:"Posts",handler:()=>{window.location.href="/blog/2020/0rlbasic/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-beast-engine",title:"Beast Engine",description:"AI-Native Game Engine",section:"Projects",handler:()=>{window.location.href="/projects/Beast/"}},{id:"projects-machine-learning-for-blast-performance-analysis-in-mining",title:"Machine Learning for Blast Performance Analysis in Mining",description:"Automated Blast Rating from Drone Footage",section:"Projects",handler:()=>{window.location.href="/projects/Blasting/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%32%32%39%35%35%32%39%34%38%32@%71%71.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ssh022-s","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@shihaosun-p5o","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>