<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Policy Gradient Method | Shuai Liu </title> <meta name="author" content="Shuai Liu"> <meta name="description" content="Reinforcement Learning basic concepts"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://liushuai26.github.io/blog/2020/5pg/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shuai</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Policy Gradient Method</h1> <p class="post-meta"> Created in December 25, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/rl"> <i class="fa-solid fa-tag fa-sm"></i> RL</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Value Based: A policy was generated directly from the value function e.g. using $\epsilon$-greedy</p> <p>In this lecture we will directly parametrize the policy</p> <p>Advantages:</p> <ul> <li>Better convergence properties</li> <li>Eﬀective in high-dimensional or continuous action spaces</li> <li>Can learn stochastic policies</li> </ul> <p>Disadvantages:</p> <ul> <li>Typically converge to a local rather than global optimum</li> <li>Evaluating a policy is typically ineﬃcient and high variance</li> </ul> <hr> <p>We consider the case of a stochastic, parameterized policy, $\pi_\theta$. Our goal is given policy $\pi_\theta(s,a)$ with parameters $\theta$, ﬁnd best $\theta$. <strong>But how do we measure the quality of a policy $\pi_\theta$?</strong> We use policy objective functions $J(\theta)$. We aim to maximize the expected return $J(\theta)$.</p> <p>In episodic environments we can use the <strong>start value</strong></p> \[J_1(\theta) = V^{\pi_\theta}(s_1) = \Bbb E_{\pi_\theta}[v_1]\] <p>In continuing environments we can use the <strong>average value</strong></p> \[J_{avV}(\theta) = \underset s \sum d^{\pi_\theta}(s)V^{\pi_\theta}(s)\] <p>Or the <strong>average reward per time-step</strong></p> \[J_{avR}(\theta) = \underset s \sum d^{\pi_\theta}(s) \underset a \sum \pi_\theta(s,a)\cal R^a_s\] <p>where $d^{\pi_\theta}(s)$ is <strong>stationary distribution</strong> of Markov chain for $\pi_\theta$</p> <hr> <p>Now on, Policy based reinforcement learning is an <strong>optimization</strong> problem.</p> <p>Then Find $\theta$ that maximizes $J(\theta)$ using gradient ascend, eg</p> \[\theta_{k+1} = \theta_k + \alpha \nabla_\theta J(\pi_\theta)|_{\theta_k}\] <p>The gradient of policy performance, $\nabla_\theta J(\pi_\theta)$, is called the <strong>policy gradient</strong>, and algorithms that optimize the policy this way are called <strong>policy gradient algorithms.</strong></p> <hr> <p>Score Function describes how much better or worse it is than other actions on average (relative to the current policy) in state s.</p> <p>Assume policy $\pi_\theta$ is differentiable whenever is non-zero and we know the gradient $\nabla_\theta\pi_\theta(s, a)$</p> <p><strong>Likelihood ratios</strong> exploit the following identity</p> \[\nabla_\theta\pi_\theta(s, a) = \pi_\theta(s,a)\frac {\nabla_\theta\pi_\theta(s,a)}{\pi_\theta(s,a)}\\ =\pi_\theta(s, a)\nabla_\theta\log\pi_\theta(s, a)\] <p>The <strong>score function</strong> is $\nabla_\theta\log\pi_\theta(s, a)$</p> <p>in discrete action spaces, Softmax Policy, score function is</p> \[\nabla_\theta\log\pi_\theta(s, a) = \phi(s,a)-\Bbb E_{\pi_\theta}[\phi(s, \cdot)]\] <hr> <h3 id="policy-gradient-theorem">Policy Gradient Theorem</h3> <p>The policy gradient theorem generalizes the likelihood ratios approach to multi-step MDPs</p> <p>$Q^\pi(s,a)$ here is long-term value, could be reward $R_t$.</p> <blockquote> <p>Theorem</p> <p>For any differentiable policy $\pi_\theta(s, a)$</p> <p>for any of the policy objective functions $J=J_1,J_{avR}, \frac 1 {1-\gamma}J_{avR}$, the policy gradient is \(\nabla_\theta J(\theta) = \Bbb E_{\pi_\theta}[\nabla_\theta\log{\pi_\theta}(s,a)Q^{\pi_\theta}(s,a)]\)</p> </blockquote> <hr> <h3 id="implementing-the-simplest-policy-gradient">Implementing the Simplest Policy Gradient</h3> <p>(For full code click <a href="https://github.com/openai/spinningup/blob/master/spinup/examples/pg_math/1_simple_pg.py" rel="external nofollow noopener" target="_blank">here</a>.)</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make core of policy network
</span><span class="n">obs_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="n">obs_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">logits</span> <span class="o">=</span> <span class="nf">mlp</span><span class="p">(</span><span class="n">obs_ph</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="n">hidden_sizes</span><span class="o">+</span><span class="p">[</span><span class="n">n_acts</span><span class="p">])</span>

<span class="c1"># make action selection op (outputs int actions, sampled from policy)
</span><span class="n">actions</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">squeeze</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="nf">multinomial</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">logits</span><span class="p">,</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>This block builds a feedforward neural network categorical policy.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make loss function whose gradient, for the right data, is policy gradient
</span><span class="n">weights_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">act_ph</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">placeholder</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="bp">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">int32</span><span class="p">)</span>
<span class="n">action_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">one_hot</span><span class="p">(</span><span class="n">act_ph</span><span class="p">,</span> <span class="n">n_acts</span><span class="p">)</span>
<span class="n">log_probs</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="nf">reduce_sum</span><span class="p">(</span><span class="n">action_masks</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nf">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">tf</span><span class="p">.</span><span class="nf">reduce_mean</span><span class="p">(</span><span class="n">weights_ph</span> <span class="o">*</span> <span class="n">log_probs</span><span class="p">)</span>
</code></pre></div></div> <p>In this block, we build a “loss” function for the policy gradient algorithm. When the right data is plugged in, the gradient of this loss is equal to the policy gradient. The right data means a set of (state, action, weight) tuples collected while acting with environment according to the current policy, where the <strong>weight</strong> for a state-action pair is the reward $R_t$.</p> <hr> <h3 id="reducing-variance-using-a-critic">Reducing Variance using a Critic</h3> <p>Monte-Carlo policy gradient still has high variance.</p> <p>We use a <strong>critic</strong> to estimate the action-value function</p> \[Q_w(s, a) \approx Q^{\pi_\theta}(s, a)\] <p>Actor-critic algorithms maintain two sets of parameters</p> <p><strong>Critic</strong> Updates action-value function parameters $w$</p> <p><strong>Actor</strong> Updates policy parameters $\theta$, in direction suggested by critic</p> <p>Actor-critic algorithms follow an <em>approximate</em> policy gradient</p> \[\nabla_\theta J(\theta) \approx \Bbb E_{\pi_\theta}[\nabla_\theta\log\pi_\theta(s,a)\ Q_w(s,a)] \\ \Delta_\theta = \alpha \nabla_\theta\log \pi_\theta(s,a)\ Q_w(s,a)\] <hr> <h3 id="bias-in-actor-critic-algorithms">Bias in Actor-Critic Algorithms</h3> <p>we can avoid introducing any bias if we choose value function approximation carefully.</p> <hr> <h3 id="reducing-variance-using-a-baseline">Reducing Variance using a baseline</h3> <p>because EGLP lemma, We have $\underset {a_t\sim \pi_\theta} E [\nabla_\theta\log\pi_\theta(a_t|s_t)b(s_t)] = 0$</p> <p>This allows us to add or subtract any number of terms like this from our expression for the policy gradient, without changing it in expectation:</p> \[\nabla_\theta J(\theta) = \Bbb E_{\pi_\theta}\left[\nabla_\theta\log{\pi_\theta}(s,a)\left(Q^{\pi_\theta}(s,a)-b(s)\right)\right]\] <p>A good baseline is the state value function $b(s) = V^{\pi_\theta}(s)$</p> <p>So we can rewrite the policy gradient using the advantage function $A^\pi(s_t, a_t) = Q^\pi(s_t, a_t) - V^\pi(s_t)$</p> <p>describes how much better or worse it is than other actions on average (relative to the current policy).</p> <hr> <h3 id="estimating-the-advantage-function">Estimating the Advantage function</h3> <p>The advantage function can signiﬁcantly reduce variance of policy gradient</p> <p>So the critic should really estimate the advantage function</p> <p>For example, by estimating both $V^{\pi_\theta}(s)$ and $Q^{\pi_\theta}(s,a)$</p> <p>Using two function approximators and two parameter vectors,</p> \[V_w(s) \approx V^{\pi_\theta}(s)\\ Q_w(s,a) \approx Q^{\pi_\theta}(s,a)\\ A(s,a) = Q_w(s,a) - V_w(s)\] <p>And updating both value functions by e.g. TD learning</p> <hr> <p>Reference:</p> <p>Reinforcement Learning An Introduction</p> <p>https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html</p> <p>http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/7dqn/">DQN</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/6pga/">Policy Gradient Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/4fa/">Function Approximation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/3mfp/">Model-Free Prediction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/2dp/">Dynamic Programming</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shuai Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-dqn",title:"DQN",description:"DQN and it's improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2020/7dqn/"}},{id:"post-policy-gradient-algorithms",title:"Policy Gradient Algorithms",description:"Introduce Policy Gradient Algorithms includes VPG, PPO, A3C, DDPG, TD3, SAC.",section:"Posts",handler:()=>{window.location.href="/blog/2020/6pga/"}},{id:"post-policy-gradient-method",title:"Policy Gradient Method",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/5pg/"}},{id:"post-function-approximation",title:"Function Approximation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/4fa/"}},{id:"post-model-free-prediction",title:"Model-Free Prediction",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/3mfp/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/2dp/"}},{id:"post-multi-armed-bandits-exploration-and-exploitation",title:"Multi-armed Bandits: exploration and exploitation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/1mab/"}},{id:"post-reinforcement-learning-basic-concepts",title:"Reinforcement Learning Basic Concepts",description:"Reinforcement Learning basic concepts and Finite Markov Decision Processes",section:"Posts",handler:()=>{window.location.href="/blog/2020/0rlbasic/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-beast-engine",title:"Beast Engine",description:"AI-Native Game Engine",section:"Projects",handler:()=>{window.location.href="/projects/Beast/"}},{id:"projects-machine-learning-for-blast-performance-analysis-in-mining",title:"Machine Learning for Blast Performance Analysis in Mining",description:"Automated Blast Rating from Drone Footage",section:"Projects",handler:()=>{window.location.href="/projects/Blasting/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%68%75%61%69.%6C%69%75.%61%69@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/liushuai26","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@aicat-","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>