<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DQN | Shihao Sun </title> <meta name="author" content="Shihao Sun"> <meta name="description" content="DQN and it's improvement."> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ssh022-s.github.io/blog/2020/7dqn/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shihao</span> Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">DQN</h1> <p class="post-meta"> Created in December 26, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/rl"> <i class="fa-solid fa-tag fa-sm"></i> RL</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="dqn">DQN</h2> <p>Goal is maximizes future rewards.</p> <p>Define the future discounted return at time t as $R_t = \sum^T_{t’=t}\gamma^{t’-t}r_{t’}$. T is the time-step at which the games terminates. (Assume the future rewards are discounted by a factor of $\gamma$ per time-step.)</p> <p>Define the optimal action-value function $Q^*(s, a)$ as the maximum expected return. ($\cal E$ : Emulator)</p> \[Q^*(s, a) = \Bbb E_{s' \sim \cal E}[r+\gamma \underset {a'} \max Q^*(s', a')|s, a]\] <p>function approximator to estimate the action-value function $Q(s, a; \theta) \approx Q^*(s, a)$</p> <p>non-linear function approximator: neural network $Q(s, a; \theta) \approx Q^*(s, a)$</p> <p>Loss function $L_i(\theta_i)$</p> \[L_i(\theta_i) = \Bbb E_{s,a \sim \rho(\cdot)}[(y_i - Q(s, a;\theta_i))^2]\\ y_i = \Bbb E_{s' \sim \cal E}[r + \gamma \underset {a'} \max Q(s', a'; \theta_{i-1})|s, a]\] <p>$y_i$ is the target.</p> \[\nabla_{\theta_i}L_i(\theta_i) = \Bbb E_{s,a \sim \rho(\cdot);s'\sim \cal E}[(r+\gamma \underset {a'} \max Q(s', a'; \theta_{i-1})-Q(s, a;\theta_i))\nabla_{\theta_i}Q(s, a;\theta_i)]\] <p>greedy strategy $a = \underset a \max Q(s, a; \theta)$ with probability $1 - \varepsilon $ and selects a random action with probability $\varepsilon$</p> <p>two mechanisms make it work:</p> <p><strong>Fixed Q-targets</strong>:</p> <ul> <li>Using a separate network with a fixed parameter (let’s call it w-) for estimating the TD target.</li> <li>At every $\tau$ step, we copy the parameters from our DQN network to update the target network.</li> </ul> <p><strong>Experience replay</strong>: making more efficient use of observed experience:</p> <ul> <li>Avoid forgetting previous experiences.</li> <li>Reduce correlations between experiences.</li> </ul> <hr> <h2 id="double-q-learning">Double Q-learning</h2> <p>Q-learning, it is known to sometimes learn unrealistically high action values because it includes a maximization step over estimated action values, which tends to prefer overestimated to underestimated values.</p> <p>Overoptimize due to estimation errors.</p> <p>The idea of Double Q-learning is to reduce overestimations by decomposing the max operation in the target into action selection and action evaluation. Although not fully decoupled, the target network in the DQN architecture provides a natural candidate for the second value function, without having to introduce additional networks. <strong>We therefore propose to evaluate the greedy policy according to the online network, but using the target network to estimate its value.</strong></p> <p>In reference to both Double Q-learning and DQN, we refer to the resulting algorithm as Double DQN.</p> \[Y_t^{DQN} = R_{t+1}+\gamma \underset a \max Q(S_{t+1}, a; \theta_t^-)\] <p>It’s update is the same as for DQN, but replacing the target $Y_t^{DQN}$ with</p> \[Y_t^{DDQN} = R_{t+1}+\gamma Q(S_{t+1}, \underset a {\arg\max}Q(S_{t+1}, a;\theta_t), \theta_t^-)\] <p>Why Double Q-learning work?</p> <p>Theorem 1 in the paper.</p> <hr> <h2 id="dueling-dqn">Dueling DQN</h2> <p>The key insight behind our new architecture is that for many states, it is unnecessary to estimate the value of each action choice.</p> <p>The proposed network architecture, which we name the dueling architecture, explicitly separates the representation of state values and (state-dependent) action advantages. The dueling architecture consists of two streams that represent the value and advantage functions, while sharing a common convolutional feature learning module. The two streams are combined via a special aggregating layer to produce an estimate of the state-action value function Q.</p> <p>From the expressions for advantage $Q^\pi(s, a) = V^\pi(s) + A^\pi(s, a)$ and state-value $V^\pi(s) = \Bbb E_{a \sim \pi(s)}[Q^\pi(s, a)]$, it follows that $\Bbb E_{a \sim \pi(s)}[A^\pi(s, a)] = 0$. Moreover, for a deterministic policy, $a^* = \arg\max_{a’\in\cal A}Q(s, a’)$, it follows that $Q(s, a^<em>)=V(s)$ and hence $A(s, a^</em>) = 0$.</p> <p>Using the deﬁnition of advantage, we might be tempted to construct the aggregating module as follows:</p> \[Q(s, a;\theta, \alpha, \beta) = V(s;\theta,\beta)+A(s,a;\theta,\alpha) \tag 7\] <p>$V(s;\theta,\beta)$ is a scalar and $A(s,a;\theta,\alpha)$ is a $|\cal A|$-dimensional vector.</p> <p>Equation above is unidentifiable in the sense that given Q we cannot recover V and A uniquely.</p> <p>To address this issue of identifiability, we can force the advantage function estimator to have zero advantage at the chosen action.</p> \[Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta)+ \left(A(s,a;\theta,\alpha)-\underset {a'\in|\cal A|} \max A(s,a';\theta,\alpha)\right) \tag 8\] <p>Now, for $a^<em>=\arg\max_{a’\in\cal A}Q(s,a’;\theta,\alpha,\beta) = \arg\max_{a’\in\cal A}A(s,a’;\theta,\alpha)$, we obtain $Q(s, a^</em>;\theta,\alpha, \beta)=V(s;\theta,\beta)$. Hence, the stream $V(s;\theta,\beta)$ provides an estimate of the value function, while the other stream produces an estimate of the advantage function.</p> <p>An alternative module replaces the max operator with an average:</p> \[Q(s,a;\theta,\alpha,\beta) = V(s;\theta,\beta)+ \left(A(s,a;\theta,\alpha)- \frac 1 {|\cal A|} \underset {a'} \sum A(s,a';\theta,\alpha)\right) \tag 9\] <p>On the one hand this loses the original semantics of V and A because they are now off-target by a constant, but on the other hand it increases the stability of the optimization: with (9) the advantages only need to change as fast as the mean, instead of having to compensate any change to the optimal action’s advantage in (8).</p> <p>The advantage of the dueling architecture lies partly in its ability to learn the state-value function efﬁciently. With every update of the Q values in the dueling architecture, the value stream V is updated – this contrasts with the updates in a single-stream architecture where only the value for one of the actions is updated, the values for all other actions remain untouched. This more frequent updating of the value stream in our approach allocates more resources to V , and thus allows for better approximation of the state values, which in turn need to be accurate for temporal difference-based methods like Q-learning to work.</p> <hr> <h2 id="prioritized-experience-replay">Prioritized Experience Replay</h2> <p>The key idea is that an RL agent can learn more effectively from some transitions than from others.</p> <p>In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error.</p> <p>TD error $\delta$, which indicates how ‘surprising’ or unexpected the transition is: speciﬁcally, how far the value is from its next-step bootstrap estimate.</p> <p>In particular, we propose to more frequently replay transitions with high expected learning progress, as measured by the magnitude of their temporal-difference (TD) error. This prioritization can lead to a loss of diversity, which we alleviate with stochastic prioritization, and introduce bias, which we correct with importance sampling. Our resulting algorithms are robust and scalable, which we demonstrate on the Atari 2600 benchmark suite, where we obtain faster learning and state-of-the-art performance.</p> <p>we use absolute TD error because it appears to be a reasonable approximation to the “usefulness” of sample</p> <p>a KL-based one in Rainbow DQN</p> <p>We can not keep a tally of all the magnitude of TD errors updated. DeepMind proposes a far more computationally efficient alternative of only updating the $\delta$ terms for items that are actually sampled during the minibatch gradient updates. Since we have to compute $\delta$ anyway to get the loss, we might as well use those to change the priorities.</p> <p>Next, given the absolute TD terms, how do we get a probability distribution for sampling? DeepMind proposes two ways of getting priorities, denoted as $p_i$:</p> <ul> <li>indirect: A <em>rank</em> based method: $p_i = \frac 1 {rank(i)}$ where $rank(i)$ is the rank of transition $i$ when the replay memory is sorted according to $|\delta|$</li> <li>direct: A <em>proportional</em> variant: $p_i = |\delta_i|+\epsilon$, where $\epsilon$ is a small constant ensuring that the sample has some non-zero probability of being drawn.</li> </ul> <p>PER initializes $p_i$ according to the maximum priority of any priority thus far, thus favoring those terms during sampling later.</p> <p>From either of these, we can easily get a probability distribution: $P(i) = \frac {p_i^\alpha} {\sum_k p_k^\alpha}$ The exponent $\alpha$ determines how much prioritization is used, with $\alpha = 0$ corresponding to the uniform case.</p> <p>Since the buffer size N can be quite large (e.g., one million), DeepMind uses special data structures to reduce the time complexity of certain operations. For the proportional-based variant, which is what OpenAI implements, a sum-tree data structure is used to make both updating and sampling O(logN) operations.</p> <p>The estimation of the expected value with stochastic updates relies on those updates corresponding to the same distribution as its expectation. Prioritized replay introduces bias because it changes this distribution in an uncontrolled fashion, and therefore changes the solution that the estimates will converge to (even if the policy and state distribution are fixed). We can correct this bias by using importance-sampling (IS) weights.</p> <p>How do we apply importance sampling? We use the following weights:</p> \[w_i = (\frac 1 N \cdot \frac 1 {P(i)})^\beta\] <p>The $\frac 1 N$ part is because of the current experience replay size.</p> <p>I think the distribution DeepMind is talking about (“same distribution as its expectation”) above is the distribution of samples that are obtained when sampling uniformly at random from the replay buffer.</p> <p><em>PER over-samples those with high priority</em>, so the importance sampling correction should <em>down-weight</em> the impact of the sampled term, which it does by scaling the gradient term so that the gradient has “less impact” on the parameters.</p> <p>the importance sampling in PER is to correct the over-sampling with respect to the uniform distribution.</p> <p>See more in https://danieltakeshi.github.io/2019/07/14/per/</p> <hr> <h2 id="rainbow">Rainbow</h2> <p>Rainbow =</p> <p>Double DQN + Prioritized experience replay + Dueling network architecture + Learning from multi-step bootstrap targets + Distributional Q-learning + Noisy DQN</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/6pga/">Policy Gradient Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/5pg/">Policy Gradient Method</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/4fa/">Function Approximation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/3mfp/">Model-Free Prediction</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/2dp/">Dynamic Programming</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shihao Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-dqn",title:"DQN",description:"DQN and it's improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2020/7dqn/"}},{id:"post-policy-gradient-algorithms",title:"Policy Gradient Algorithms",description:"Introduce Policy Gradient Algorithms includes VPG, PPO, A3C, DDPG, TD3, SAC.",section:"Posts",handler:()=>{window.location.href="/blog/2020/6pga/"}},{id:"post-policy-gradient-method",title:"Policy Gradient Method",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/5pg/"}},{id:"post-function-approximation",title:"Function Approximation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/4fa/"}},{id:"post-model-free-prediction",title:"Model-Free Prediction",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/3mfp/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/2dp/"}},{id:"post-multi-armed-bandits-exploration-and-exploitation",title:"Multi-armed Bandits: exploration and exploitation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/1mab/"}},{id:"post-reinforcement-learning-basic-concepts",title:"Reinforcement Learning Basic Concepts",description:"Reinforcement Learning basic concepts and Finite Markov Decision Processes",section:"Posts",handler:()=>{window.location.href="/blog/2020/0rlbasic/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-beast-engine",title:"Beast Engine",description:"AI-Native Game Engine",section:"Projects",handler:()=>{window.location.href="/projects/Beast/"}},{id:"projects-machine-learning-for-blast-performance-analysis-in-mining",title:"Machine Learning for Blast Performance Analysis in Mining",description:"Automated Blast Rating from Drone Footage",section:"Projects",handler:()=>{window.location.href="/projects/Blasting/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%32%32%39%35%35%32%39%34%38%32@%71%71.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ssh022-s","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@shihaosun-p5o","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>