<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reinforcement Learning Basic Concepts | Shihao Sun </title> <meta name="author" content="Shihao Sun"> <meta name="description" content="Reinforcement Learning basic concepts and Finite Markov Decision Processes"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://ssh022-s.github.io/blog/2020/0rlbasic/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Shihao</span> Sun </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reinforcement Learning Basic Concepts</h1> <p class="post-meta"> Created in July 25, 2020 </p> <p class="post-tags"> <a href="/blog/2020"> <i class="fa-solid fa-calendar fa-sm"></i> 2020 </a>   ·   <a href="/blog/category/rl"> <i class="fa-solid fa-tag fa-sm"></i> RL</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The main characters of RL are the <strong>agent</strong> and the <strong>environment</strong>. The environment is the world that the agent lives in and interacts with. At every step of interaction, the agent sees a (possibly partial) observation of the state of the world, and then decides on an action to take. The environment changes when the agent acts on it, but may also change on its own.</p> <p>The agent also perceives a <strong>reward</strong> signal from the environment, a number that tells it how good or bad the current world state is. The goal of the agent is to maximize its cumulative reward, called <strong>return</strong>. Reinforcement learning methods are ways that the agent can learn behaviors to achieve its goal.</p> <p>additional terminology:</p> <ul> <li>states and observations,</li> <li>action spaces,</li> <li>policies,</li> <li>trajectories,</li> <li>different formulations of return,</li> <li>the RL optimization problem,</li> <li>and value functions.</li> </ul> <h3 id="states-and-observations">States and Observations</h3> <p>A <strong>state</strong> is a complete description of the state of the world. There is no information about the world which is hidden from the state. An <strong>observation</strong> is a partial description of a state, which may omit information.</p> <h3 id="action-spaces">Action Spaces</h3> <p>The set of all valid actions in a given environment is often called the <strong>action space</strong>. Some environments, like Atari and Go, have <strong>discrete action spaces</strong>, where only a finite number of moves are available to the agent. Other environments, like where the agent controls a robot in a physical world, have <strong>continuous action spaces</strong>. In continuous spaces, actions are real-valued vectors.</p> <h3 id="policies">Policies</h3> <p>A <strong>policy</strong> is the agent’s behavior. It is a map from state to action.</p> <p>A <strong>policy</strong> is a rule used by an agent to decide what actions to take.</p> <p>It can be deterministic, in which case it is usually denoted by $\mu$: $a_t = \mu(s_t)$ or it may be stochastic, in which case it is usually denoted by $\pi$: $a_t = \pi(\cdot|s_t)$ or $\pi(a|s) = \Bbb P[A_t=a|S_t=s]$</p> <h4 id="deterministic-policies">Deterministic Policies</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pi_net</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">(</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">obs_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">(),</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Tanh</span><span class="p">(),</span>
              <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">act_dim</span><span class="p">)</span>
            <span class="p">)</span>

<span class="n">obs_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">as_tensor</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
<span class="n">actions</span> <span class="o">=</span> <span class="nf">pi_net</span><span class="p">(</span><span class="n">obs_tensor</span><span class="p">)</span>
</code></pre></div></div> <h4 id="stochastic-policies">Stochastic Policies</h4> <p>The two most common kinds of stochastic policies in deep RL are <strong>categorical policies</strong> and <strong>diagonal Gaussian policies</strong>.</p> <p><strong>Categorical</strong> policies can be used in discrete action spaces, while diagonal <strong>Gaussian</strong> policies are used in continuous action spaces.</p> <p>A categorical policy is like a classifier over discrete actions. one final linear layer that gives you logits for each action, followed by a softmax to convert the logits into probabilities.</p> <p>Two key computations are centrally important for using and training stochastic policies:</p> <ul> <li>sampling actions from the policy,</li> <li>and computing log likelihoods of particular actions, $\log\pi_\theta(a|s)$.</li> </ul> <h3 id="trajectories">Trajectories</h3> <p>A trajectory $\tau$ is a sequence of states and actions in the world,</p> <p>Trajectories are also frequently called <strong>episodes</strong> or <strong>rollouts</strong>.</p> <h3 id="rewards">Rewards</h3> <ul> <li>A <strong>reward</strong> $R_t$ is a scalar feedback signal</li> <li>Indicates how well agent is doing at step $t$</li> <li>The agent’s job is to maximize cumulative reward</li> </ul> <p>Reinforcement learning is based on the <strong>reward hypothesis</strong></p> <p><strong>Definition (Reward Hypothesis):</strong> <em>All</em> goals can be described by the maximization of expected cumulative reward.</p> <p>One kind of return is the <strong>finite-horizon undiscounted return</strong>.</p> <p>Another kind of return is the <strong>infinite-horizon discounted return</strong>, which is the sum of all rewards <em>ever</em> obtained by the agent, but discounted by how far off in the future they’re obtained.</p> <p>Why would we ever want a discount factor, though? Don’t we just want to get <em>all</em> rewards? We do, but the discount factor is both intuitively appealing and mathematically convenient. On an intuitive level: cash now is better than cash later. Mathematically: an infinite-horizon sum of rewards <a href="https://en.wikipedia.org/wiki/Convergent_series" rel="external nofollow noopener" target="_blank">may not converge</a> to a finite value, and is hard to deal with in equations. But with a discount factor and under reasonable conditions, the infinite sum converges.</p> <p>we frequently set up algorithms to optimize the undiscounted return, but use discount factors in estimating <strong>value functions</strong>:</p> <ol> <li>We typically measure the “performance” of an RL algorithm by looking at the average episodic undiscounted sum of rewards, but we implement algorithms that use discount factors in value function learning, and increase probabilities of actions in proportion to advantage functions that use discount factors.</li> <li>From the above, you might get the impression that we’re measuring undiscounted return, but optimizing discounted return. The truth is, we aren’t exactly optimizing for either! If we were truly optimizing for discounted return, there would be a discount factor included in the objective function for each state, corresponding to what timestep in a trajectory that state occurred. But, in typical implementations, we ignore that discount factor and weight each state’s contribution equally.</li> </ol> <hr> <h3 id="the-rl-problem">The RL Problem</h3> <p>Whatever the choice of return measure (whether infinite-horizon discounted, or finite-horizon undiscounted), and whatever the choice of policy, the goal in RL is to select a policy which maximizes <strong>expected return</strong> when the agent acts according to it.</p> <p>Let’s suppose that both the environment transitions and the policy are stochastic. In this case, the probability of a T-step trajectory is: \(P(\tau|\pi) = \rho_0(s_0)\prod_{t=0}^{T-1}P(s_{t+1}|s_t, a_t)\pi(a_t|s_t).\) The expected return (for whichever measure), denoted by $J(\pi)$, is then: \(J(\pi) = \int_\tau P(\tau|\pi)R(\tau) = E_{\tau \sim \pi}[R(\tau)]\) The central optimization problem in RL can then be expressed by \(\pi^* = \arg \max_\pi J(\pi),\) with $\pi^*$ being the <strong>optimal policy</strong></p> <hr> <p>An RL agent may include one or more of these components:</p> <ul> <li>Policy: agent’s behavior function</li> <li>Value function: how good is each state and/or action</li> <li>Model: agent’s representation of the environment</li> </ul> <h3 id="value-functions">Value Functions</h3> <ul> <li> <p>Value function is a prediction of future reward</p> </li> <li> <p>Used to evaluate the goodness/badness of states</p> </li> <li> <p>And therefore to select between actions, e.g. \(V_\pi(s) = \Bbb E_\pi[R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + ...|S_t=s]\)</p> </li> </ul> <p>There are four main functions of note here.</p> <ol> <li>The <strong>On-Policy Value Function</strong>, $V^\pi(s) = E_{\tau\sim\pi}[R(\tau)|s_0=s]$</li> <li>The <strong>On-Policy Action-Value Function</strong>, $Q^\pi(s, a) = E_{\tau\sim\pi}[R(\tau)|s_0=s, a_0=a]$</li> <li>The <strong>Optimal Value Function</strong>, $V^*(s) = \max_\pi E_{\tau\sim\pi}[R(\tau)|s_0=s]$</li> <li>The <strong>Optimal Action-Value Function</strong>, $Q^*(s, a) = \max_\pi E_{\tau\sim\pi}[R(\tau)|s_0=s, a_0=a]$</li> </ol> <p>two key connections between the value function and the action-value function: \(V^\pi(s) = E_{a\sim\pi}[Q^\pi(s, a)], \\ V^*(s) = \max_a Q^*(s, a).\) The optimal action: \(a^*(s) = \arg \max_aQ^*(s, a).\)</p> <p><strong>Model</strong></p> <ul> <li> <p>A <strong>model</strong> predicts what the environment will do next</p> </li> <li> <p>$P$ predicts the next state</p> </li> <li> <p>$R$ predicts the next (immediate) reward, e.g. \(P_{ss'}^a = \Bbb P[S_{t+1}=s'|S_t=s, A_t=a] \\ R_s^a = \Bbb E[R_{t+1}|S_t=s, A_t = a]\)</p> </li> </ul> <hr> <h3 id="bellman-equations">Bellman Equations</h3> <p>The basic idea behind the Bellman equations is this:</p> <blockquote> <p>The value of your starting point is the reward you expect to get from being there, plus the value of wherever you land next.</p> </blockquote> <p>The Bellman equations for the on-policy value functions are \(V^\pi(s) = \underset {s'\sim P} {\underset {a\sim\pi} E}[r(s, a)+\gamma V^\pi(s')], \\ Q^\pi(s, a) = \underset {s'\sim P} E[r(s, a) + \gamma \underset {a'\sim\pi}E[Q^\pi(s', a')]]\)</p> <h3 id="advantage-functions">Advantage Functions</h3> <p>Sometimes in RL, we don’t need to describe how good an action is in an absolute sense, but only how much better it is than others on average. That is to say, we want to know the relative <strong>advantage</strong> of that action. We make this concept precise with the <strong>advantage function.</strong> \(A^\pi(s, a) = Q^\pi(s, a)-V^\pi(s).\)</p> <hr> <hr> <h2 id="formalism">Formalism</h2> <p>So far, we’ve discussed the agent’s environment in an informal way, but if you try to go digging through the literature, you’re likely to run into the standard mathematical formalism for this setting: <strong>Markov Decision Processes</strong> (MDPs). An MDP is a 5-tuple, $\langle S, A, R, P, \rho_0 \rangle$.</p> <p>The name Markov Decision Process refers to the fact that the system obeys the <a href="https://en.wikipedia.org/wiki/Markov_property" rel="external nofollow noopener" target="_blank">Markov property</a>: transitions only depend on the most recent state and action, and no prior history.</p> <h3 id="history-and-state">History and state</h3> <p>The <strong>history</strong> is the sequence of observations, actions, rewards \(H_t = O_1, R_1, A_1, ..., A_{t-1}, O_t, R_t\) <strong>State</strong> is the information used to determine what happens next.</p> <p>state is a function of the history: \(S_t = f(H_t)\)</p> <h3 id="information-state">Information State</h3> <p>An <strong>information state</strong> (a.k.a <strong>Markov state</strong>) contains all useful information from the history.</p> <p>A state $S_t$ is <strong>Markov</strong> if and only if \(\Bbb P[S_{t+1}|S_t] = \Bbb P[S_{t+1}|S_1, ..., S_t]\)</p> <ul> <li> <p>The future is independent of the past given the present \(H_{1:t} \rightarrow S_t \rightarrow H_{t+1:\infin}\)</p> </li> <li> <p>Once the state is known, the history may be thrown away</p> </li> <li> <p>i.e. The state is a sufficient statistic of the future</p> </li> <li> <p>The environment state $S_t^e$ is Markov</p> </li> <li> <p>The history $H_t$ is Markov</p> </li> </ul> <hr> <p><em>Markov decision processes</em> formally describe an environment for reinforcement learning</p> <p>Where the environment is <em>fully observable</em></p> <p><strong>Full observability:</strong> agent directly observes environment state \(O_t = S_t^a = S_t^e\)</p> <ul> <li>Agent state = environment state = information state</li> <li>Formally, this is a <strong>Markov decision process</strong> (MDP)</li> </ul> <p><strong>Partial observability:</strong> agent indirectly observes environment.</p> <ul> <li>Now agent state $\not=$ environment state</li> <li>Formally this is a <strong>partially observable Markov decision process</strong> (POMDP)</li> </ul> <p>Definition $G_t$​</p> <p>The <em>return</em> $G_t$ is the total discounted reward from time-step $t$. \(G_t = R_{t+1} + \gamma R_{t+2} +\ ...=\underset {k=0} {\overset \infty \sum} \gamma^kR_{t+k+1}\)</p> <ul> <li>The <em>discount</em> $\gamma \in [0, 1]$ is the present value of future rewards</li> <li>The value of receiving reward R after k+1 time-steps is $\gamma^kR$</li> <li>This values immediate reward above delayed reward.</li> <li>$\gamma$ close to 0 leads to “myopic” evaluation</li> <li>$\gamma$ close to 1 leads to “far-sighted” evaluation</li> </ul> <p>The value function $v(s)$ gives the long-term value of state $s$</p> <p>Definition <strong>state value function</strong></p> <p>The <em>state value function</em> v(s) of an MRP is the expected return starting from state $s$ \(v(s) = \Bbb E[G_t|S_t=s]\) <strong>Bellman Equation for MRPs</strong></p> <p>The value function can be decomposed into two parts:</p> <ul> <li>immediate reward $R_{t+1}$</li> <li>discounted value of successor state $\gamma v(S_{t+1})$</li> </ul> \[v(s) = \Bbb E[G_t|S_t=s] = \Bbb E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]\] <hr> <p><strong>Markov Decision Process</strong></p> <p>A Markov decision process (MDP) is a Markov reward process with decisions. It is an <em>environment</em> in which all states are Markov.</p> <p><strong>Definition</strong></p> <p>A <em>Markov Decision Process</em> is a tuple $&lt;S, A, P, R, \gamma&gt;$</p> <ul> <li> <p>$S$ is a finite set of states</p> </li> <li> <p>A is a finite set of actions</p> </li> <li> <p>P is a state transition probability matrix, $P_{ss’}^a=\Bbb P[S_{t+1}=s’|S_t=s, A_t=a]$</p> </li> <li> <p>R is a reward function, $R_s^a = \Bbb E[R_{t+1}|S_t=s, A_t=a]$</p> </li> <li> <p>$\gamma$ is a discount factor $\gamma \in [0, 1]$</p> </li> </ul> <p><strong>Policies</strong> for MDP</p> <p><strong>Definition</strong></p> <p>A <em>policy</em> $\pi$ is a distribution over actions given states, \(\pi(a|s) = \Bbb P[A_t=a|S_t=s]\)</p> <ul> <li>A policy fully defines the behavior of an agent</li> <li>MDP policies depend on the current state (not the history)</li> </ul> <p><strong>Value Function</strong> for MDP</p> <p><strong>Definition</strong></p> <p>The <em>state-value</em> function $v_\pi(s)$ of an MDP is the expected return starting from state s, and then following policy $\pi$ \(v_\pi(s) = \Bbb E_\pi[G_t|S_t=s]\) The <em>action-value</em> function $q_\pi(s, a)$ is the expected return starting from state s, taking action a, and then following policy $\pi$ \(q_\pi(s, a)=\Bbb E_\pi[G_t|S_t=s, A_t=a]\)</p> <hr> <p>Bellman Expectation Equation</p> <p>The state-value function can again be decomposed into immediate reward plus discounted value of successor state, \(v_\pi(s) = \Bbb E_\pi[R_{t+1}+\gamma v_\pi(S_{t+1})|S_t=s]\) The action-value function can similarly be decomposed, \(q_\pi(s, a)=\Bbb E_\pi[R_{t+1}+\gamma q_\pi(S_{t+1}, A_{t+1})|S_t=s, A_t=a]\)</p> <hr> <h3 id="formula-derivation">Formula derivation:</h3> <p>Expected discounted return: \(\begin{align} G_t &amp; \dot= R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \gamma^3R_{t+4}+ \dots \\ &amp; = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \gamma^2R_{t+4}+ \dots) \\ &amp; = R_{t+1} + \gamma G_{t+1} \end{align}\) where $\gamma$ is a parameter, $0 \le \gamma \le 1$, called the discount rate.</p> <p>Bellman equation for $v_\pi$ \(\begin{align} v_\pi(s) &amp;\dot=\Bbb E_\pi[G_t|S_t=s]\\ &amp;= \Bbb E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s]\\ &amp;= \underset a \sum\pi(a|s)\underset {s'} \sum\underset r \sum p(s', r|s,a)[r+\gamma \Bbb E_\pi[G_{t+1}|S_{t+1}=s']]\\ &amp;=\underset a \sum \pi(a|s) \underset {s',r} \sum p(s', r|s,a)[r+\gamma v_\pi(s')],\ for\ all\ s \in \cal S \end{align}\)</p> <hr> <p>We denote all the <em>optimal policies</em> by $\pi_*$.</p> <p>$\pi \ge \pi’$ if and only if $v_\pi(s) \ge v_{\pi’}(s)$ for all $s \in \cal S$</p> <p><em>optimal state-value function</em></p> <p>$v_*(s) \dot= \underset \pi \max v_\pi(s)$, for all $s \in \cal S$</p> <p><em>optimal action-value function</em>, denoted $q_*$:</p> <p>$q_*(s, a) \dot=\underset \pi \max q_\pi(s, a)$, for all $s \in \cal S$ and $a \in \cal A(s)$.</p> <p>$q_<em>$ in terms of $v_</em>$:</p> \[q_*(s, a) = \Bbb E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a]\] <hr> <p>Bellman optimality equation for $v_*$</p> \[\begin{align} v_\pi(s) &amp;= \underset {a \in \cal A(s)} \max q_{\pi_*}(s, a)\\ &amp;= \underset a \max \Bbb E_{\pi_*}[G_t|S_t=s, A_t=a]\\ &amp;= \underset a \max \Bbb E_\pi[R_{t+1}+\gamma G_{t+1}|S_t=s, A_t=a]\\ &amp;= \underset a \max \Bbb E[R_{t+1} + \gamma v_*(S_{t+1}) | S_t=s, A_t=a] \\ &amp;=\underset a \max\underset {s',r} \sum p(s', r|s,a)[r+\gamma v_\pi(s')] \end{align}\] <p>Bellman optimality equation for $q_*$</p> \[q_*(s, a) = \Bbb E[R_{t+1} + \gamma \underset {a'} \max q_*(S_{t+1}, a') | S_t=s, A_t=a]\\ = \underset {s', r} \sum p(s', r|s, a)[r+ \gamma \underset {a'} \max q_*(s', a')].\] <p>Reference:</p> <p>Reinforcement Learning An Introduction</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/7dqn/">DQN</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/6pga/">Policy Gradient Algorithms</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/5pg/">Policy Gradient Method</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/4fa/">Function Approximation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2020/3mfp/">Model-Free Prediction</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Shihao Sun. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-projects",title:"Projects",description:"",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-dqn",title:"DQN",description:"DQN and it's improvement.",section:"Posts",handler:()=>{window.location.href="/blog/2020/7dqn/"}},{id:"post-policy-gradient-algorithms",title:"Policy Gradient Algorithms",description:"Introduce Policy Gradient Algorithms includes VPG, PPO, A3C, DDPG, TD3, SAC.",section:"Posts",handler:()=>{window.location.href="/blog/2020/6pga/"}},{id:"post-policy-gradient-method",title:"Policy Gradient Method",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/5pg/"}},{id:"post-function-approximation",title:"Function Approximation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/4fa/"}},{id:"post-model-free-prediction",title:"Model-Free Prediction",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/3mfp/"}},{id:"post-dynamic-programming",title:"Dynamic Programming",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/2dp/"}},{id:"post-multi-armed-bandits-exploration-and-exploitation",title:"Multi-armed Bandits: exploration and exploitation",description:"Reinforcement Learning basic concepts",section:"Posts",handler:()=>{window.location.href="/blog/2020/1mab/"}},{id:"post-reinforcement-learning-basic-concepts",title:"Reinforcement Learning Basic Concepts",description:"Reinforcement Learning basic concepts and Finite Markov Decision Processes",section:"Posts",handler:()=>{window.location.href="/blog/2020/0rlbasic/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-beast-engine",title:"Beast Engine",description:"AI-Native Game Engine",section:"Projects",handler:()=>{window.location.href="/projects/Beast/"}},{id:"projects-machine-learning-for-blast-performance-analysis-in-mining",title:"Machine Learning for Blast Performance Analysis in Mining",description:"Automated Blast Rating from Drone Footage",section:"Projects",handler:()=>{window.location.href="/projects/Blasting/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%32%32%39%35%35%32%39%34%38%32@%71%71.%63%6F%6D","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/ssh022-s","_blank")}},{id:"socials-youtube",title:"YouTube",section:"Socials",handler:()=>{window.open("https://youtube.com/@shihao sun","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>