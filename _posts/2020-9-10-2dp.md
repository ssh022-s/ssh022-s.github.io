---
layout: post
title:  "Dynamic Programming"
date:   2020-9-10 01:20:00 +0800
description: Reinforcement Learning basic concepts
tags: 
categories: RL
featured: false
---

What is dynamic programming?

The term dynamic programming (DP) refers to a collection of algorithms that can be used to **compute optimal policies** given a **perfect model of the environment as a Markov decision process (MDP)**. 

**Dynamic**: sequential or temporal component to the problem

**Programming**: optimizing a “program”, i.e. a policy

Dynamic programming assumes full knowledge of the MDP

Dynamic Programming is a very general solution method for problems which have two properties: 

* Optimal substructure
  - Principle of optimality applies
  - Optimal solution can be decomposed into subproblems
* Overlapping subproblems
  * Subproblems recur many times
  * Solutions can be cached and reused

Markov decision processes satisfy both properties

* Bellman equation gives recursive decomposition
* Value function stores and reuses solutions

### Policy Evaluation (Prediction)

We consider how to compute the state-value function $v_\pi$ for an arbitrary policy $\pi$. This is called policy evaluation in the DP literature. We also refer to it as the prediction problem.


$$
v_\pi(s) \dot = \underset a \sum \pi(a|s) \underset {s',r} \sum p(s',r|s,a)[r+\gamma v_\pi(s')]
$$


where $\pi(a\|s)$ is the probability of taking action $a$ in state $s$ under policy $\pi$, and the expectations are subscripted by $\pi$ to indicate that they are conditional on $\pi$ being followed.


$$
\begin{align}
v_{k+1}(s)\ & \dot=\  \Bbb E_\pi[R_{t+1}+\gamma v_k(S_{t+1})|S_t=s]\\
& = \underset a \sum \pi(a|s)\underset {s',r}\sum p(s',r|s,a)[r+\gamma v_k(s')]
\end{align}
$$


The sequence $\{v_k\}$ can be shown in general to converge to $v_\pi$ as $k\rightarrow \infty$ under the same conditions that guarantee the existence of $v_\pi$.







### Policy Improvement

Consider a deterministic policy, $a = \pi(s)$

We can *improve* the policy by acting greedily


$$
\pi'(s) = \underset {a \in A} {\arg \max}\     q_\pi(s, a)
$$


Let $\pi$ and $\pi'$ be any pair of deterministic policies such that, for all $s\in S$, 


$$
q_\pi(s, \pi'(s)) \geq v_\pi(s)
$$


Then the policy $\pi'$ must be as good as, or better than, $\pi$. That is, it must obtain greater or equal expected return from all states  $s\in S$:


$$
v_{\pi'} \geq v_\pi(s)
$$


new *greedy* policy, $\pi'$, given by


$$
\begin{align}
\pi'(s)\ & \dot =\ \underset a {\arg \max}\ q_{\pi}(s, a)\\
& = \underset a {\arg \max}\ \Bbb E[R_{t+1}+\gamma v_{\pi}(S_{t+1})\ |\ S_t=s, A_t=a]\\
& = \underset a {\arg \max}\ \underset {s',r} \sum p(s',r|s,a)[r+\gamma v_\pi(s')]

\end{align}
$$



### Value Iteration

If we know the solution to subproblems $v_*(s')$, then solution $v_*(s)$ can be found by one-step lookahead


$$
v_*(s) \leftarrow \underset {a\in A} \max R_s^a + \gamma \underset {s'\in S}\sum P_{ss'}^av_*(s')
$$


Value iteration is obtained simply by turning the Bellman optimality equation into an update rule.

Value iteration effectively combines, in each of its sweeps, one sweep of policy evaluation and one sweep of policy improvement. 

### Asynchronous Dynamic Programming

Three simple ideas for asynchronous dynamic programming:

* In-place dynamic programming
* Prioritized sweeping
* Real-time dynamic programming

**In-place dynamic programming**

In-place value iteration only stores one copy of value function. that is, with each new value immediately overwriting the old one. It usually converges faster than the two-copies version, because it uses new data as soon as they are available. 

**Prioritized Sweeping**

Use magnitude of Bellman error to guide state selection. Backup the state with the largest remaining Bellman error. Can be implemented eﬃciently by maintaining a priority queue.

**Real-time dynamic programming**

Idea: only states that are relevant to agent.

After each time-step agent interactive with environment $S_t, A_t, R_{t+1}$. Only use those experience to Backup:


$$
v(S_t) \leftarrow \underset {a\in A} \max \left(R_{S_t}^a + \gamma \underset {s'\in S}\sum P_{S_ts'}^av(s')\right)
$$

### Generalized Policy Iteration

Almost all reinforcement learning methods are well described as GPI. That is, all have identiﬁable policies and value functions, with the policy always being improved with respect to the value function and the value function always being driven toward the value function for the policy, as suggested by the diagram to the right. If both the evaluation process and the improvement process stabilize, that is, no longer produce changes, then the value function and policy must be optimal. The value function stabilizes only when it is consistent with the current policy, and the policy stabilizes only when it is greedy with respect to the current value function.

## Summary

Policy evaluation refers to the (typically) iterative computation of the value functions for a given policy. 

Policy improvement refers to the computation of an improved policy given the value function for that policy. 

Putting these two computations together, we obtain policy iteration and value iteration, the two most popular DP methods. 

It is not necessary to perform DP methods in complete sweeps through the state set. Asynchronous DP methods are in-place iterative methods that update states in an arbitrary order.

